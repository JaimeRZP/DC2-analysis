{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Clusters & Velocity: A HackUrDC2 Story\n",
    "\n",
    "Author: Julien Peloton [@JulienPeloton](https://github.com/JulienPeloton)  \n",
    "Last Verifed to Run: 2019-01-06  \n",
    "Estimated running time: < 15 min.\n",
    "\n",
    "**Context**\n",
    "\n",
    "This notebook illustrates the basics of accessing the cosmoDC2 catalog through Apache Spark as well as how to select useful samples of data to study galaxy clusters. \n",
    "\n",
    "**Science results or validation?**\n",
    "\n",
    "While it describes some of the science results you can get with the DC2 catalog (e.g. halo mass-velocity dispersion relation), it shifted towards the validation side, especially on the halo mass distribution. So it is a bit of both! After going through this notebook, you should be able to:\n",
    "\n",
    "- Load and efficiently access the cosmoDC2 catalog with Apache Spark\n",
    "- Apply cuts to the catalog using Spark SQL functionalities\n",
    "- Have several example of quality cuts and validation procedures \n",
    "- Derive scientific results on galaxy clusters\n",
    "- Distribute the computation and the plotting routine to be faster!\n",
    "\n",
    "**Table of contents:**\n",
    "- Loading cosmoDC2 data with Apache Spark\n",
    "- Halo mass distribution\n",
    "  - issue: https://github.com/LSSTDESC/DC2-analysis/issues/55\n",
    "- Galaxy clusters and velocity\n",
    "  - A few individual galaxy clusters\n",
    "  - A wider region of space\n",
    "  - Mean velocity as a function of redshift\n",
    "- The velocity dispersionâ€“halo mass relation\n",
    "  - issue: https://github.com/LSSTDESC/DC2-analysis/issues/57\n",
    "- Visualising masses in the cosmoDC2 Universe\n",
    "- Outro: Towards large data sets with Apache Spark\n",
    "\n",
    "**Logistics:** \n",
    "\n",
    "This notebook is intended to be run through the JupyterHub NERSC interface with the desc-pyspark kernel. The kernel is automatically installed in your environment when you use the kernel setup script:\n",
    "\n",
    "```bash\n",
    "source /global/common/software/lsst/common/miniconda/kernels/setup.sh\n",
    "```\n",
    "\n",
    "**Thanks to:**\n",
    "\n",
    "[@aphearin](https://github.com/aphearin), [@cwwalter](https://github.com/cwwalter), [@dkorytov](https://github.com/dkorytov), [@rmandelb](https://github.com/rmandelb), [@salmanhabib](https://github.com/salmanhabib), [@yymao](https://github.com/yymao), for their help in understanding better how DC2 catalog was made, and solving issues while making this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Generator, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf , PandasUDFType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DC2 data with Spark\n",
    "\n",
    "Apache Spark has no efficient PySpark connector to read data in hdf5 file. Therefore we first converted the cosmoDC2 data set into parquet (similar to what DPDD tools offer). For the purpose of this notebook, we only convert a few columns of interest. The file is accessible at NERSC for DESC members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data\n",
    "fn = \"/global/cscratch1/sd/peloton/cosmodc2/xyz_v1.1.4_mass_and_mag.parquet\"\n",
    "\n",
    "# Load the data - this a lazy operation, no data movement yet!\n",
    "df = spark.read.format(\"parquet\").load(fn)\n",
    "\n",
    "# Let's inspect the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Number of objects in the catalog\n",
    "print(\"Number of rows: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look a some mass values. Apache Spark provides filter mechanisms, which you can use to speed up data access if you only need a certain chunks of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reject synthetic halos - see for example discussion in \n",
    "# https://github.com/LSSTDESC/cosmodc2/issues/82\n",
    "cols = [\"halo_mass\", \"stellar_mass\", \"blackHoleMass\", \"halo_id\"]\n",
    "df.filter(\"halo_id > 0\").select(cols).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `halo_mass` is duplicated for all the members of the same halo. \n",
    "We can also easily look at statistics about individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the stellar_mass and halo_mass distributions\n",
    "df.select([\"stellar_mass\", \"halo_mass\", \"redshift\"]).describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more about Apache Spark in the context of LSST DESC, see e.g. [LSSTDESC/DC2-production](https://github.com/LSSTDESC/DC2-production/tree/master/Notebooks), [LSSTDESC/DC2-analysis](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/object_spark_1_intro.ipynb), [LSSTDESC/desc-spark](https://github.com/LSSTDESC/desc-spark), or connect to [AstroLab Software](https://astrolabsoftware.github.io/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halo mass distribution in DC2\n",
    "\n",
    "To start this journey, let's look at the distribution of halo masses in the DC2 data set.\n",
    "We will select only clusters with positive halo-ids (i.e. we reject the synthetic halos that are added to host the ultra-faint galaxies). In addition, we will create two other populations: low-z (0.0 < z < 0.2), and high-z (2.5 < z < 3.1) clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cache the data for speeding up later computations\n",
    "df_mass = df.filter('is_central == True').filter(\"halo_id > 0\").select(\"halo_mass\").cache()\n",
    "\n",
    "# Populations based on redshift range\n",
    "df_mass_lowz = df_mass.filter(\"redshift < 0.2\")\n",
    "df_mass_highz = df_mass.filter(\"redshift > 2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of clusters is rather high, we will capitalize on the fact that we are doing computation in parallel.\n",
    "The way to be faster is to distribute the computation which leads to the data to be plotted. Histograms are particularly easy to distribute:\n",
    "- Load the data set across several partitions. Each partition has about 128 MB of data.\n",
    "- Apply filters on lines and select columns (order does not matter as Spark will choose the optimal way). Partitions will be processed in parallel. If you have more partitions than workers (typically CPU), there will be a partition queue.\n",
    "- With the remaining data in each partition, build an histogram per partition.\n",
    "- Reduce to the driver all partition histograms by summing them up. You have the final histogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the text above\n",
    "Image('histo_spark.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write such a method to be applied on each Spark partition to compute histograms in parallel (each would contain only a fraction of the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_hist(partition: Iterator, bins : np.ndarray = None) -> Generator:\n",
    "    \"\"\" Produce histograms from partition data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    partition : Iterator\n",
    "        Iterator containing partition data *[x].\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    Generator yielding histograms for each partition. \n",
    "    Counts is an array of dimension nbins - 1.\n",
    "    \"\"\"\n",
    "    # Unwrap the iterator\n",
    "    mass = [*partition]\n",
    "    \n",
    "    (counts, edges) = np.histogram(mass, bins)\n",
    "    \n",
    "    yield counts\n",
    "\n",
    "def parallel_hist(df: DataFrame, bins : np.ndarray, col: str = None) -> np.ndarray:\n",
    "    \"\"\" Build an histogram of the data in parallel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Apache Spark DataFrame.\n",
    "    bins : 1D numpy array\n",
    "        The bin edges, including the rightmost edge.\n",
    "    col : str, optional\n",
    "        Column to select. If None, the DataFrame is\n",
    "        expected to have only one column. Default is None.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    hist : 1D numpy array\n",
    "        Full histogram over `bins` for the selected DataFrame column.\n",
    "        hist has dimension len(bins) - 1.\n",
    "\n",
    "    \"\"\"\n",
    "    # Go from DataFrame to RDD world\n",
    "    if col is None:\n",
    "        rdd = df.rdd\n",
    "    else:\n",
    "        rdd = df.select(col).rdd\n",
    "        \n",
    "    # Build the histograms in each partition\n",
    "    hist = rdd\\\n",
    "        .mapPartitions(lambda partition: partition_hist(partition, bins))\\\n",
    "        .reduce(lambda x, y: x+y)\n",
    "\n",
    "    return hist\n",
    "\n",
    "# Binning of mass in log space\n",
    "bins = np.logspace(8, 15, 50)\n",
    "\n",
    "# Build histograms for the 3 populations\n",
    "dataplot = []\n",
    "for index, dataframe in enumerate([df_mass, df_mass_lowz, df_mass_highz]):\n",
    "    # This is the crucial part - build the plot data in parallel, and\n",
    "    # reduce the final result to the driver. Scalable to Billions of points!\n",
    "    dataplot.append(parallel_hist(dataframe, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "labels = [\"All redshifts\", \"0.0 < z < 0.2\", \"2.5 < z < 3.1\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "for index in range(len(dataplot)):\n",
    "    plt.loglog(bins[:-1], dataplot[index], label=labels[index])\n",
    "    print('Entries for {} : {} ({:.1f}%)'.format(\n",
    "        labels[index], \n",
    "        np.sum(dataplot[index]), \n",
    "        np.sum(dataplot[index])/np.sum(dataplot[0])*100))\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(2.5e10, 1e15)\n",
    "plt.title('Halo Mass distribution in cosmoDC2 (v1.1.4)', fontsize=20)\n",
    "plt.xlabel(r'${\\rm M}_h [{\\rm M}_\\odot]$', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A weird feature!\n",
    "\n",
    "We selected the clusters based on their `halo_id`, and to avoid double counting we took only entries corresponding to central galaxies (all galaxies for a given `halo_id` have the same `halo_mass`).\n",
    "\n",
    "If now we look closely at the `halo_mass` distribution values, we observe a weird feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.filter('is_central == True').filter(\"halo_id > 0\").select('halo_mass')\n",
    "df2 = df.filter('is_central == True').filter(\"halo_id > 0\").select('halo_mass').distinct()\n",
    "\n",
    "print(\"All halo_mass values: {}\".format(df1.count()))\n",
    "print(\"All distinct halo_mass values: {}\".format(df2.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There many different halos (i.e. different position on sky) with exactly the same mass! Note that we already select central galaxies to grab the value of the halo mass. Let's look at both distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build plots in parallel - millions of points!\n",
    "data1 = parallel_hist(df1, bins)\n",
    "data2 = parallel_hist(df2, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.loglog(bins[:-1], data1, label=\"All mass values\")\n",
    "plt.loglog(bins[:-1], data2, label=\"Unique mass values\")\n",
    "plt.title(\"Oooops!\", fontsize=20)\n",
    "plt.xlim(2.5e10, 1e15)\n",
    "plt.xlabel(r'${\\rm M}_h [{\\rm M}_\\odot]$', fontsize=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means many halos with mass lower than $10^{13} {\\rm M}_\\odot$ have the same mass.\n",
    "\n",
    "**Question**: Is that expected?\n",
    "\n",
    "**Answer:** Yes! This is the so-called mass quantization in N-body simulations: all halo masses must by definition be some integer times the particle mass value used in the simulations (a few times 10^9 solar masses). For a detailed discussion, see https://github.com/LSSTDESC/DC2-analysis/issues/55. OK, this one was easy to solve...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galaxy clusters and velocity in DC2\n",
    "\n",
    "### A few individual galaxy clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on some selected galaxy clusters. We will select 5 rich ones ($n_{gal} > 50$), and look at their spatial morphology (in comoving cartesian coordinates), plus the 3D velocity of their members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reject synthetic halos - see for example discussion in \n",
    "# https://github.com/LSSTDESC/cosmodc2/issues/82\n",
    "df_halo = df.filter(\"halo_id > 0\")\n",
    "\n",
    "# Group by halo_id, and count members\n",
    "df_halo_grouped = df_halo.groupBy(\"halo_id\").count().filter(\"count > 50\")\n",
    "df_halo_grouped.show(5)\n",
    "\n",
    "# Take the 5 first\n",
    "halos = df_halo_grouped.select(\"halo_id\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect halos data into Pandas DataFrame. Why using Pandas? to show that you can go from Spark world to Pandas world easily (and also for fun):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select position, velocity, stellar mass and redshift\n",
    "cols_3d = [\"position_x\", \"position_y\", \"position_z\", \"velocity_x\", \"velocity_y\", \"velocity_z\"]\n",
    "cols = cols_3d + [\"stellar_mass\", \"halo_mass\", \"redshift\"]\n",
    "pandas_dfs = []\n",
    "for halo in halos:\n",
    "    # Keep only members of the halo\n",
    "    halo_id = np.array(halo)[0]\n",
    "    df_members = df_halo.filter(\"halo_id == {}\".format(halo_id)).select(cols)\n",
    "    \n",
    "    # Halo members\n",
    "    pandas_df = df_members.toPandas()\n",
    "    \n",
    "    # Central galaxy\n",
    "    pandas_df_center = df_members.filter(\"is_central == True\").toPandas()\n",
    "    \n",
    "    pandas_dfs.append([pandas_df, pandas_df_center])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that `toPandas()` sends back the data from all the executors to the lonely poor driver. So be careful to not use that on TB of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15, 35))\n",
    "fig.suptitle('Selected Halos', y=0.93, fontsize=30)\n",
    "\n",
    "for index, halo, pdfs in zip(range(len(halos)), halos, pandas_dfs):\n",
    "    \n",
    "    # First plot column: position of members, color coded with stellar mass\n",
    "    ax = fig.add_subplot(5, 2, 2*index + 1, projection='3d')\n",
    "    mass_scale = pdfs[0].stellar_mass / pdfs[0].stellar_mass.max() * 200\n",
    "    halo_id = np.array(halo)[0]\n",
    "    zmean = pdfs[1].redshift.mean()\n",
    "    halo_mass = pdfs[1].halo_mass.mean()\n",
    "    \n",
    "    ax.scatter(\n",
    "        pdfs[0].position_x, \n",
    "        pdfs[0].position_y, \n",
    "        pdfs[0].position_z, s=mass_scale, c=mass_scale)\n",
    "    ax.scatter(\n",
    "        pdfs[1].position_x, \n",
    "        pdfs[1].position_y, \n",
    "        pdfs[1].position_z, \n",
    "        s=200, color=\"red\", marker=\"x\")\n",
    "    ax.set_title(\n",
    "        'Halo ID {} \\n (z $\\sim$ {:.2f}, $M_h = {:.2e} M_\\odot$)'.format(\n",
    "            halo_id, zmean, halo_mass))\n",
    "    ax.set_xlabel(r'x [Mpc/h]', labelpad=20)\n",
    "    ax.set_ylabel(r'y [Mpc/h]', labelpad=20)\n",
    "    ax.set_zlabel(r'z [Mpc/h]', labelpad=20)\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "    # Second plot column: position of members + 3D velocity, \n",
    "    # color coded with stellar mass.\n",
    "    ax = fig.add_subplot(5, 2, 2*index + 2, projection='3d')\n",
    "    ax.scatter(\n",
    "        pdfs[0].position_x, \n",
    "        pdfs[0].position_y, \n",
    "        pdfs[0].position_z, s=mass_scale, c=mass_scale)\n",
    "    q = ax.quiver(\n",
    "        pdfs[0].position_x, \n",
    "        pdfs[0].position_y, \n",
    "        pdfs[0].position_z, \n",
    "        pdfs[0].velocity_x, \n",
    "        pdfs[0].velocity_y, \n",
    "        pdfs[0].velocity_z, length=0.0002, color='k')\n",
    "    ax.scatter(\n",
    "        pdfs[1].position_x, \n",
    "        pdfs[1].position_y, \n",
    "        pdfs[1].position_z, \n",
    "        s=200, color=\"red\", marker=\"x\")\n",
    "    ax.set_title(\n",
    "        'Halo ID {} \\n (z $\\sim$ {:.2f}, $M_h = {:.2e} M_\\odot$)'.format(\n",
    "            halo_id, zmean, halo_mass))\n",
    "    ax.set_xlabel(r'x [Mpc/h]', labelpad=20)\n",
    "    ax.set_ylabel(r'y [Mpc/h]', labelpad=20)\n",
    "    ax.set_zlabel(r'z [Mpc/h]', labelpad=20)\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color code: \n",
    "- yellow = big = more massive, \n",
    "- purple = small = less massive\n",
    "\n",
    "Note that for the 2nd and 4th halos the velocities seem to be aligned along an axis as if they were following some flow. \n",
    "On the third halo, galaxies seem to rotate around the center. \n",
    "\n",
    "### A wider region of space\n",
    "\n",
    "Let's have a look at a wider region of space. We focus on a cube of 15 Mpc size (~ 2000 halos), compute and display the mean velocity for each halo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "cols = [\"halo_id\", \"position_x\", \"position_y\", \"position_z\", \"redshift\", \"is_central\"]\n",
    "cols_vel = [\"velocity_x\", \"velocity_y\", \"velocity_z\"]\n",
    "cols_mass = [\"halo_mass\"]\n",
    "df_sub = df.select(cols + cols_mass + cols_vel).filter(\"halo_id > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean position of our cube of data\n",
    "df_stat = df_sub.select([\"position_x\", \"position_y\", \"position_z\"]).describe()\n",
    "means = df_stat.collect()[1]\n",
    "\n",
    "m_x = float(means['position_x'])\n",
    "m_y = float(means['position_y'])\n",
    "m_z = float(means['position_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our data cube\n",
    "Mpc = 15.\n",
    "\n",
    "# Keep only this region of space\n",
    "dfcut = df_sub.filter(\n",
    "    (F.abs(df_sub.position_x - m_x) < Mpc) &\n",
    "    (F.abs(df_sub.position_y - m_y) < Mpc) &\n",
    "    (F.abs(df_sub.position_z - m_z) < Mpc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by halos and compute the mean velocity\n",
    "df_disp = dfcut.groupBy(\"halo_id\").agg({\n",
    "    \"velocity_x\": 'mean',\n",
    "    \"velocity_y\": 'mean',\n",
    "    \"velocity_z\": 'mean'})\n",
    "\n",
    "# Add back original columns, and \n",
    "# select only a few columns for the plot\n",
    "data_joined = dfcut.join(df_disp, \"halo_id\")\\\n",
    "    .filter(\"is_central == True\")\\\n",
    "    .select(\n",
    "    \"avg(velocity_x)\", \"avg(velocity_y)\", 'avg(velocity_z)', \n",
    "    \"position_x\", \"position_y\", \"position_z\", \n",
    "    'redshift', 'halo_mass')\n",
    "\n",
    "# Show a few columns \n",
    "data_joined.show(5)\n",
    "\n",
    "# Go to Pandas world for the plot\n",
    "pandas_df = data_joined.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "q = ax.quiver(\n",
    "    pandas_df.position_x, \n",
    "    pandas_df.position_y, \n",
    "    pandas_df.position_z, \n",
    "    pandas_df[\"avg(velocity_x)\"], \n",
    "    pandas_df[\"avg(velocity_y)\"], \n",
    "    pandas_df[\"avg(velocity_z)\"], length=0.0005, color='k')\n",
    "ax.scatter(\n",
    "    pandas_df.position_x, \n",
    "    pandas_df.position_y, \n",
    "    pandas_df.position_z, \n",
    "    s=pandas_df.halo_mass/pandas_df.halo_mass.max()*400, c='r')\n",
    "plt.title(\n",
    "    'Mean 3D velocity of halos (z $\\sim$ {:.2f})'.format(pandas_df.redshift.mean()))\n",
    "ax.set_xlabel(r'x [Mpc/h]', labelpad=20)\n",
    "ax.set_ylabel(r'y [Mpc/h]', labelpad=20)\n",
    "ax.set_zlabel(r'z [Mpc/h]', labelpad=20)\n",
    "ax.tick_params(axis='both', labelsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the 3D mean velocity (black arrow) for each halo contained in this data cube (15 Mpc size). We also superimposed the halo mass (red circle). The bigger is the circle, the more massive is the halo. It is interesting to notice that most massive halos are surrounded by halos with large velocities, they seem to flow towards them.\n",
    "\n",
    "With more times, that would be interesting to study other properties and the environment of those halos and see whether there is a correlation with the velocities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean velocity as a function of redshift\n",
    "\n",
    "To end this journey, let's have a look at the mean velocity distribution as a function of redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift range\n",
    "redshift_start = 0.0\n",
    "redshift_stop = 3.0\n",
    "redshift_step = 0.5\n",
    "redshift_window = 0.1\n",
    "values = np.arange(redshift_start, redshift_stop, redshift_step)\n",
    "\n",
    "# start at 0.2 because stat is poor at very low redshift\n",
    "values[0] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = []\n",
    "for index, start in enumerate(values[:]):\n",
    "    dfcut = df_sub.filter(\"redshift > {}\".format(start))\\\n",
    "        .filter(\"redshift <= {}\".format(start + redshift_window))\n",
    "    \n",
    "    # Group data by halos and compute the mean velocity\n",
    "    df_disp = dfcut.groupBy(\"halo_id\").agg({\n",
    "        \"velocity_x\": 'mean',\n",
    "        \"velocity_y\": 'mean',\n",
    "        \"velocity_z\": 'mean'})\n",
    "\n",
    "    # Add back original columns, and \n",
    "    # select only a few columns for the plot\n",
    "    data_joined = dfcut.join(df_disp, \"halo_id\")\\\n",
    "        .filter(\"is_central == True\")\\\n",
    "        .select(\"avg(velocity_x)\", \"avg(velocity_y)\", 'avg(velocity_z)')\n",
    "    pdfs.append(data_joined.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.title(\"Mean 3D velocity of halos\")\n",
    "for index, pandas_df in enumerate(pdfs):\n",
    "    # simple mean\n",
    "    meanv = np.sqrt(\n",
    "        pandas_df[\"avg(velocity_x)\"]**2 + \n",
    "        pandas_df[\"avg(velocity_y)\"]**2 + \n",
    "        pandas_df[\"avg(velocity_z)\"]**2)\n",
    "    \n",
    "    # histogram\n",
    "    plt.hist(\n",
    "        meanv, bins=400, \n",
    "        label=\"{:.1f} < z < {:.1f}\".format(values[index], values[index] + redshift_window), \n",
    "        histtype=\"step\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(0, 6000)\n",
    "\n",
    "plt.xlabel(\"Mean 3D velocity of halo [km/s]\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to notice that the peak of the distribution shifts towards larger values for high redshift. The distributions are also broader at high redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The velocity dispersionâ€“halo mass relation\n",
    "\n",
    "In this section we compute the velocity dispersionâ€“halo mass relation. The principle is similar to the M-$\\sigma$ relation for stars around black holes. The idea is to highlight the fact that gravitational interaction/friction between galaxies has an effect for the cluster galaxy evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev_pop\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def normv(vx: Any, vy: Any, vz: Any) -> pd.Series:\n",
    "    \"\"\" Compute the 3D velocity norm.\n",
    "    \n",
    "    To be used as a User Defined Function for Spark.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    v{i} : 1D array, or string, or DataFrame column\n",
    "        Velocity data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    One-dimensional ndarray with axis labels (including time series).\n",
    "    \"\"\"\n",
    "    v = np.sqrt(vx**2 + vy**2 + vz**2)\n",
    "    return pd.Series(v)\n",
    "\n",
    "# We reject synthetic halos - see for example discussion in \n",
    "# https://github.com/LSSTDESC/cosmodc2/issues/82\n",
    "# In addition we select halo members with stellar mass > 5e10 M_o.\n",
    "df_filt = df.filter(\"halo_id > 0\").filter(\"stellar_mass > 5e10\")\n",
    "\n",
    "# Add a column which is the 3D velocity norm\n",
    "df_meanv = df_filt\\\n",
    "    .withColumn(\"normv\", normv(\"velocity_x\", \"velocity_y\", \"velocity_z\"))\\\n",
    "    .select([\"normv\", \"halo_mass\", \"is_central\", \"halo_id\", \"redshift\"])\n",
    "\n",
    "# Group data by halos and compute the velocity dispersion\n",
    "df_disp = df_meanv.groupBy(\"halo_id\").agg(stddev_pop(\"normv\").alias(\"stdv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back the original DataFrame columns\n",
    "# and select only central member for halo \n",
    "# (unique halo mass and velocity dispersion for a halo)\n",
    "data_joined = df_meanv.join(df_disp, \"halo_id\")\\\n",
    "    .filter(\"is_central == True\")\\\n",
    "    .filter(\"stdv > 0\")\\\n",
    "    .select(\"stdv\", \"halo_mass\", 'redshift')\\\n",
    "    .dropna()\n",
    "data_joined.show(5)\n",
    "\n",
    "# Collect the data from the executors to the driver\n",
    "data = data_joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel, mass, redshift = np.transpose(data)\n",
    "print(\"Number of entries: {}\".format(len(vel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the velocity dispersion vs halo mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "cs = plt.hexbin(mass, vel, gridsize=150, xscale=\"log\", yscale=\"log\", mincnt=1)\n",
    "\n",
    "plt.xlabel(r'${\\rm M}_h [{\\rm M}_\\odot]$', fontsize=30)\n",
    "plt.ylabel(r'$\\sigma_v$ [km/s]', fontsize=30)\n",
    "\n",
    "plt.colorbar(cs, label=\"halos per bin\")\n",
    "\n",
    "title = \"Velocity dispersionâ€“halo mass relation\\n\"\n",
    "hypotheses = r\"(${\\rm M}_*$ > $5\\times10^{10}{\\rm M}_\\odot$, $n_{gal/halo}$ > 1, 0 < z < 3)\"\n",
    "plt.title(title + hypotheses, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that there is a good relationship between the two. One can also see regular stripes at some halo masses. Let's inspect the 1D mass distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect these stripes\n",
    "import seaborn as sns\n",
    "\n",
    "joint_kws = dict(gridsize=200, mincnt=1)\n",
    "g = sns.jointplot(\n",
    "    np.log10(mass), \n",
    "    np.log10(vel), \n",
    "    height=8, space=0.5,\n",
    "    kind='hex', xlim=(11.5, 15), ylim=(0, 3), color='k',\n",
    "    joint_kws=joint_kws, \n",
    "    marginal_kws=dict(bins=200, rug=True))\n",
    "\n",
    "g.set_axis_labels(r'$\\log({\\rm M}_h \\, [{\\rm M}_\\odot])$', r'$\\log(\\sigma_v \\, [km/s])$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum, the peaks in the 1D halo mass distribution seem to be regularly spaced in log space. In order to study more in details this effect, I created another notebook: [hackurdc2_spark_halos_appendix.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/u/JulienPeloton/hackurdc2_spark_halos/contributed/hackurdc2_spark_halos_appendix.ipynb). This is also discussed in detailed in [LSSTDESC/DC2-analysis/issues/57](https://github.com/LSSTDESC/DC2-analysis/issues/57).\n",
    "\n",
    "Long story short: The tooth shape is expected from how galaxies in the Universe Machine are sampled into the Outer Rim halo light cone. This will put a limit on the accuracy with which cosmoDC2 observations could constrain the halo mass of stacked galaxy sample, and future simulations may implement a bin-free sampling method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the data\n",
    "\n",
    "Following e.g. [1602.00611](https://arxiv.org/abs/1602.00611) (see section 4.1), we can model the mean relation between velocity dispersion and halo mass at a given redshift using a simple power-law of the form:\n",
    "\n",
    "$$\\sigma_v(\\rm{M}_h) = \\alpha \\Big( \\dfrac{\\rm{M}_h}{10^{14}\\rm{M}_\\odot} \\Big)^{\\beta}$$\n",
    "\n",
    "and the dependency in redshift is given by\n",
    "\n",
    "$$\\sigma_v(z) = \\sigma_v(0)\\sqrt{1 + z}$$\n",
    "\n",
    "Let's split our data set into redshit bins, and fit for the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "redshift_start = np.min(redshift)\n",
    "redshift_stop = 3.0\n",
    "redshift_step = 0.4\n",
    "redshift_window = 0.1\n",
    "values = np.arange(redshift_start, redshift_stop, redshift_step)\n",
    "\n",
    "# start at 0.2 because stat is poor at very low redshift\n",
    "values[0] = 0.2\n",
    "\n",
    "def vel_mass_relation(mass: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    \"\"\" Model for the mean relation between velocity dispersion \n",
    "    and halo mass at a given redshift using a simple power-law.\n",
    "    \n",
    "    The pivot mass is at 10^14 solar mass.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mass : 1D numpy array\n",
    "        List of masses\n",
    "    a, b: floats\n",
    "        Coefficient of the power law a*x**b\n",
    "       \n",
    "    Returns\n",
    "    ----------\n",
    "    out : 1D numpy array\n",
    "        Velocity dispersion according to the power law.\n",
    "    \"\"\"\n",
    "    return a * (mass / 1e14)**b\n",
    "\n",
    "# Use for later pretty print of coefficient fit.\n",
    "table = \"\"\"\n",
    "| redshift | $\\\\alpha$ [km/s] | $\\\\beta$ | entries (#) |\n",
    "|----------|------------------|----------|-------------|\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 7))\n",
    "fig.suptitle(\"Fit to the mean relation between velocity dispersion \\n and halo mass different redshifts\", y=1.1)\n",
    "\n",
    "for index, start in enumerate(values[:]):\n",
    "    mask = (redshift > start) & (redshift < start + redshift_window)\n",
    "    \n",
    "    res = curve_fit(vel_mass_relation, mass[mask], vel[mask])\n",
    "    val = res[0]\n",
    "    err = np.diag(res[1])\n",
    "    table += \"{:.2f} < z < {:.2f} | {:.2f} $\\pm$ {:.2f} | {:.2E} $\\pm$ {:.2E} | {} | \\n\".format(\n",
    "        start, start + redshift_window, val[0], err[0], val[1], err[1], np.sum(mask))\n",
    "\n",
    "    # Plot the fit\n",
    "    ax[0].loglog(mass[mask], vel_mass_relation(mass[mask], val[0], val[1]), \n",
    "                 ls='-', marker='', color=\"C{}\".format(index), \n",
    "                 label=r\"{:.2f} < z < {:.2f}\".format(start, start + redshift_window))\n",
    "    \n",
    "    # Rescale with the redshift dependency\n",
    "    # before fitting.\n",
    "    res = curve_fit(vel_mass_relation, mass[mask], vel[mask] / np.sqrt(1 + redshift[mask]))\n",
    "    val = res[0]\n",
    "    err = np.diag(res[1])\n",
    "    ax[1].loglog(mass[mask], vel_mass_relation(mass[mask], val[0], val[1]), \n",
    "                 ls='-', marker='', color=\"C{}\".format(index),\n",
    "                 label=r\"{:.2f} < z < {:.2f}\".format(start, start + redshift_window))\n",
    "    \n",
    "    for i in range(2):\n",
    "        ax[i].set_ylim(10, 1000)\n",
    "        ax[i].set_xlim(np.min(mass), np.max(mass))\n",
    "        ax[i].set_xlabel(r'${\\rm M}_h [{\\rm M}_\\odot]$', fontsize=30)\n",
    "    ax[0].set_ylabel(r'$\\sigma_v$ [km/s]', fontsize=30)\n",
    "    ax[1].set_ylabel(r'$\\sigma_v / \\sqrt{1 + z}$ [km/s]', fontsize=30)\n",
    "\n",
    "    ax[0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to print coefficient values\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to what is shown in e.g. 1602.00611 in a smaller redshift range (see section 4.1). If we look at the coefficients, $\\alpha$s are somewhat similar but there are some differences in $\\beta$s (note: on GitHub, the table is not correctly displayed... Try on the JupyterLab interface directly):\n",
    "\n",
    "| redshift | $\\alpha$ [km/s] | $\\beta$ | entries (#) |\n",
    "|----------|-----------|----------|-------------|\n",
    "0.20 < z < 0.30 | 308.53 $\\pm$ 46.48 | 5.17E-01 $\\pm$ 2.80E-04 | 691 | \n",
    "0.45 < z < 0.55 | 330.33 $\\pm$ 28.55 | 5.09E-01 $\\pm$ 1.27E-04 | 2231 | \n",
    "0.85 < z < 0.95 | 368.03 $\\pm$ 40.36 | 5.06E-01 $\\pm$ 1.04E-04 | 3506 | \n",
    "1.25 < z < 1.35 | 403.02 $\\pm$ 83.50 | 5.12E-01 $\\pm$ 1.53E-04 | 3200 | \n",
    "1.65 < z < 1.75 | 425.80 $\\pm$ 193.15 | 5.22E-01 $\\pm$ 2.76E-04 | 2236 | \n",
    "2.05 < z < 2.15 | 402.48 $\\pm$ 626.99 | 4.80E-01 $\\pm$ 7.84E-04 | 1237 | \n",
    "2.45 < z < 2.55 | 505.46 $\\pm$ 2236.88 | 5.57E-01 $\\pm$ 1.66E-03 | 698 | \n",
    "2.85 < z < 2.95 | 546.90 $\\pm$ 6745.14 | 5.54E-01 $\\pm$ 3.75E-03 | 346 |\n",
    "\n",
    "Part of the difference can come from different cuts applied for the minimal mass, or the definition of member galaxies.\n",
    "We note that the $\\alpha$ coefficients are poorly fitted for $z >1.6$. Maybe a different relation takes place at high redshift? Or we need more statistics? Or something else?\n",
    "\n",
    "However the scaling in redshift ($\\sigma_v \\propto \\sqrt{1 + z}$) seems rather good accross the whole data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Masses in the DC2 Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually plot the stellar mass - halo mass relation, or black hole mass - halo mass relation (see for example this [notebook](https://nbviewer.jupyter.org/github/LSSTDESC/DC2-analysis/blob/rendered/tutorials/extragalactic_gcr_mass_relations.nbconvert.ipynb)). Here we propose to visualize it in 3D directly! By visual inspection, one can easily have an intuition of the contribution of stars or black holes to halo masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select masses\n",
    "cols = [\"halo_id\", \"position_x\", \"position_y\", \"position_z\", \"redshift\"]\n",
    "cols_mass = [\"halo_mass\", \"stellar_mass\", \"blackHoleMass\"]\n",
    "df_sub = df.select(cols + cols_mass).filter(\"halo_id > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean position of our cube of data\n",
    "# (we could also have re-used this from a previous cell)\n",
    "df_stat = df_sub.select([\"position_x\", \"position_y\", \"position_z\"]).describe()\n",
    "means = df_stat.collect()[1]\n",
    "\n",
    "m_x = float(means['position_x'])\n",
    "m_y = float(means['position_y'])\n",
    "m_z = float(means['position_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our data cube\n",
    "Mpc = 30.\n",
    "\n",
    "dfcut = df_sub.filter(\n",
    "    (F.abs(df_sub.position_x - m_x) < Mpc) &\n",
    "    (F.abs(df_sub.position_y - m_y) < Mpc) &\n",
    "    (F.abs(df_sub.position_z - m_z) < Mpc))\n",
    "\n",
    "# Go to Pandas for the plot\n",
    "df_disp = dfcut.groupBy(\"halo_id\").agg(F.sum(\"stellar_mass\").alias(\"total_stellar_mass\"))\n",
    "data_joined = dfcut.join(df_disp, \"halo_id\")\n",
    "p = data_joined.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "\n",
    "def fmass(p: pd.DataFrame, which: str) -> pd.Series:\n",
    "    \"\"\" Compute the fraction of mass relative to \n",
    "    the halo mass (in %)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Pandas DataFrame\n",
    "        Pandas DataFrame\n",
    "    which : str\n",
    "        Column name for which we want the ratio\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    ratio: pd.Series\n",
    "        1D array with fractional masses.\n",
    "    \"\"\"\n",
    "    ratio = (p[which] / p['halo_mass']) * 100\n",
    "    return ratio\n",
    "\n",
    "\n",
    "# Stellar mass / halo mass\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_title('Fraction of stellar mass in Halos (z $\\sim$ {:.2f})'.format(p.redshift.mean()))\n",
    "\n",
    "c = ax.scatter(\n",
    "    p.position_x, \n",
    "    p.position_y, \n",
    "    p.position_z, \n",
    "    c=fmass(p, \"total_stellar_mass\"), \n",
    "    s=fmass(p, \"total_stellar_mass\"))\n",
    "plt.colorbar(c, shrink=0.75, label=r\"${\\rm M}_*/{\\rm M}_h$[%]\", orientation=\"horizontal\", pad=0.05)\n",
    "\n",
    "ax.set_xlabel(r'x [Mpc/h]', labelpad=20)\n",
    "ax.set_ylabel(r'y [Mpc/h]', labelpad=20)\n",
    "ax.set_zlabel(r'z [Mpc/h]', labelpad=20)\n",
    "plt.show()\n",
    "\n",
    "# Black hole mass / halo mass\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_title('Fraction of black hole mass in Halos (z $\\sim$ {:.2f})'.format(p.redshift.mean()))\n",
    "c = ax.scatter(\n",
    "    p.position_x, \n",
    "    p.position_y, \n",
    "    p.position_z, \n",
    "    c=fmass(p, \"blackHoleMass\"), \n",
    "    s=fmass(p, \"blackHoleMass\")*1000)\n",
    "plt.colorbar(c, shrink=0.75, label=r\"${\\rm M}_{bh}/{\\rm M}_h$[%]\", orientation=\"horizontal\", pad=0.05)\n",
    "\n",
    "ax.set_xlabel(r'x [Mpc/h]', labelpad=20)\n",
    "ax.set_ylabel(r'y [Mpc/h]', labelpad=20)\n",
    "ax.set_zlabel(r'z [Mpc/h]', labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises, most halo masses are not dominated by stellar masses ($< 5$%), and black holes only contribute to a few ($< 0.2$%)! Of course, one could use much better visualisation tools (combined with the power of Spark!), like is done in [hackurdc2_TheMovie/tutorials/cosmoDC2_TheMovie.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/u/LAL/hackurdc2_TheMovie/tutorials/cosmoDC2_TheMovie.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro: Towards large data sets with Apache Spark\n",
    "\n",
    "In this notebook, we have done science thanks to Apache Spark. Spark, is meant to be primarily used in a context of big data. One of its strength is its scalability, namely its capability of using the same piece of code regardless the underlying data volume. The performance of the code will then only depend on the resource used. E.g. for tasks without communications, execution time will be linear with data or resource.\n",
    "\n",
    "Keep in mind:\n",
    "\n",
    "- For small volume of data (< 10 GB), you will certainly hit Spark noise and burning time.\n",
    "- Spark is written in Scala, which is certainly not as specialised as C++ could be. Therefore for small volume of data, there is a chance an algorithm in Scala (Spark) would be slower than its C++ counterpart. But the Spark one is meant to run on TB of data as it was written for MB of data - which is probably not the case for the C++ code.\n",
    "- Once the data is loaded, you can decide to keep it in memory (distributed among the executors). The next iterations will then go super fast (typically disk I/O throughput is o(100) MB/s while RAM is o(10) GB/s).\n",
    "\n",
    "So in this example, the smallish vloume of data is likely to lessen Spark performance (volume is just few tens of GB here), and these tests must also be ran on hundreds of GB of data with many processors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_More on Spark in the context of DESC can be found at: https://github.com/LSSTDESC/desc-spark_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-pyspark",
   "language": "python",
   "name": "desc-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
