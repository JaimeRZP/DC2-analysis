{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIA on DC2: Using Run2.1i\n",
    "\n",
    "Owner: Bruno S.  (Duke U.)\n",
    "Date: 21/01/2020\n",
    "\n",
    "### Brief introduction\n",
    "\n",
    "Difference image analysis is the standard technique to uncover variability and transient events on images. Here we show some work done with the SN sample present in WFD.\n",
    "\n",
    "In this analysis we will find out what does the Run2.1i DIA dataset looks like.  \n",
    "For this we will use a set of templates built for the purpouse as well as difference images already calculated, their association catalogs, and see what we can find.\n",
    "\n",
    "Content here can be summarized as:\n",
    "1. Load DIAObject table and select an object\n",
    "2. Get the diaSrc IDs from the DIAObject table for that object\n",
    "3. Use a magic trick to get the dataIds that allow us to load the diaSrc catalogs for each observation of the source.  \n",
    "    These are the measurements from the detections on each image.  They are _not_ the forced photometry on all available images.\n",
    "4. Load diaSrc values for each detection.\n",
    "5. Calibrate the diaSrc instFlux measurements\n",
    "6. Plot lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import astropy.coordinates as SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "from astropy import time\n",
    "from collections import OrderedDict as Odict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.geom as afwGeom\n",
    "import lsst.geom as geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.afw.geom import makeSkyWcs\n",
    "from lsst.daf.persistence import Butler\n",
    "from lsst.obs.lsst.imsim import ImsimMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a butler for the difference image dataset for Run2.1i\n",
    "\n",
    "This dataset has been built through several steps, and the following graph shows roughly the steps involved.\n",
    "\n",
    "<img src='./plots/processing_schema.png' style=\"height:550px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexprepo = '/global/cscratch1/sd/desc/DC2/data/Run2.1i/rerun/calexp-v1' \n",
    "b = Butler(calexprepo)\n",
    "skymap = b.get('deepCoadd_skyMap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `calexp` repo we can ge the `skyMap` that is useful for analyzing the `tract` and `patch` to use. \n",
    "\n",
    "In this case we are going to work with tract-patch (from now on I will refer to this as `t+p`) `4431`,  `(1, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract = 4432\n",
    "patch = (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_info = skymap[tract]\n",
    "tract_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_patch_box = tract_info.getPatchInfo(patch).getOuterBBox()\n",
    "tract_patch_pos_list = tract_patch_box.getCorners()\n",
    "# Cast to Point2D, because pixelToSky below will refuse to work with a Point2I object.\n",
    "tract_patch_pos_list = [afwGeom.Point2D(tp) for tp in tract_patch_pos_list]\n",
    "\n",
    "wcs = tract_info.getWcs()\n",
    "corners = wcs.pixelToSky(tract_patch_pos_list)\n",
    "corners = np.array([[c.getRa().asDegrees(), c.getDec().asDegrees()] for c in corners])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = corners[:, 0]\n",
    "dec = corners[:, 1]\n",
    "min_ra, max_ra = np.min(ra), np.max(ra)\n",
    "min_dec, max_dec = np.min(dec), np.max(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coordinate borders for patch\\n  {:3.2f} < R.A. < {:3.2f} \\n {:3.2f} < Dec < {:3.2f}'.format(min_ra, max_ra, min_dec, max_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, we make the calculation of the approximated area of this patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ra, delta_dec = max_ra-min_ra, max_dec-min_dec\n",
    "area = np.cos(np.deg2rad(min_dec+delta_dec*0.5))*np.deg2rad(delta_ra)*np.deg2rad(delta_dec)*180.*180./(np.pi**2.)*u.deg*u.deg\n",
    "print('Area = {:4.3f} deg^2, or {:3.2f} arcmin^2'.format(area, area.to(u.arcmin**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the data id for this patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpId = {'tract': 4432, 'patch': '4,3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now acces to the repo of data we have for the difference images at the latest stage, the forced photometry.\n",
    "\n",
    "Using this butler we can access data from every parent repo, ranging from the `calexp` until the forced photometry results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_repo = '/global/cscratch1/sd/bos0109/templates_rect'\n",
    "diarepo = template_repo + '/rerun/diff_rect'\n",
    "assocrepo = diarepo + '/rerun/assoc_sha'\n",
    "forcerepo = assocrepo + '/rerun/forcedPhot' \n",
    "tmprepo = template_repo + '/rerun/multiband'\n",
    "\n",
    "diabutler = Butler(forcerepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check, which data types can we obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaMapper = diabutler._getDefaultMapper()\n",
    "mapper = diaMapper(root=forcerepo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "#print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to ask for the `diaSrc` detections, these are the individual sources that come from the difference images.\n",
    "Let's ask for similar datatypes stored in our shortlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[datatype for datatype in shortlist if 'dia' in datatype]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask for the `deepDiff_diaSrc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabutler.getKeys('deepDiff_diaSrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for the butler to be able to gives our dataset we need to specify some of the id components above.\n",
    "\n",
    "We have several files in the repo, and we can look for a visit and detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/deepDiff/v00159479-fg/R03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = [diabutler.get('deepDiff_diaSrc', visit=159479, detector=det).asAstropy().to_pandas() \n",
    "        for det in [19, 20, 22, 23, 25, 26]]\n",
    "srcs = pd.concat(srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a better way to do this, involving the Butler and a native functionality called `subset`.  \n",
    "\n",
    "This allow us to use a partial `dataId`, and we can afterwards chose what we want to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'deepDiff_diaSrc'\n",
    "\n",
    "# A visit number we happen to know\n",
    "# We pick raft 3 just to reduce the number of dataIds we're dealing with\n",
    "partial_data_id = {'visit': 159479, 'raftName': 'R03'}  \n",
    "\n",
    "data_refs = diabutler.subset(datasetType=dataset_type, dataId=partial_data_id)\n",
    "\n",
    "data_ids = [dr.dataId for dr in data_refs\n",
    "            if diabutler.datasetExists(datasetType=dataset_type,\n",
    "                                       dataId=dr.dataId)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the resulting `data_ids` list contents, and find that they are consistent with our previous result inspecting the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(did) for did in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = [diabutler.get('deepDiff_diaSrc', dataId=did).asAstropy().to_pandas() for did in data_ids]\n",
    "srcs = pd.concat(srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the table is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the positions of the sources, and the patch borders as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.rad2deg(srcs['coord_ra']), np.rad2deg(srcs['coord_dec']), \n",
    "            c=np.log10(srcs['base_PsfFlux_instFlux']))\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.grid()\n",
    "plt.vlines(x=[max_ra, min_ra], ymin=min_dec, ymax=max_dec)\n",
    "plt.hlines(y=[max_dec, min_dec], xmin=min_ra, xmax=max_ra)\n",
    "plt.gca().set_aspect(1./np.cos(np.mean([max_dec, min_dec])))\n",
    "plt.gca().invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple histogram of its fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(srcs['ip_diffim_forced_PsfFlux_instFlux']), histtype='step', color='k', lw=2)\n",
    "plt.xlabel('log10(PsfFlux)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the aperture fluxes, using different sizes of aperture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[acol for acol in srcs.columns if 'Aperture' in acol and 'instFlux' in acol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apertures = ['3_0', '4_5', '6_0', '9_0', '12_0', '17_0', '25_0', '35_0', '50_0', '70_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(9,9))\n",
    "\n",
    "for ax, ap in zip(axes.flatten(), apertures[:-1]):\n",
    "    colname = 'base_CircularApertureFlux_{}_instFlux'.format(ap)\n",
    "    ax.hist(np.log10(np.abs(srcs[colname])), color='k', histtype='step', lw=2)\n",
    "    ax.set_xlabel('log10(|ApFlux_{}|)'.format(ap))\n",
    "    ax.set_xlim(1, 7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making lightcurves\n",
    "\n",
    "In order to make lightcurves we will need to use the `diaObject` table. Let's open one of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = diabutler.get('deepDiff_diaObject', dataId=tpId).asAstropy().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are several columns for each one of these `diaObject` and they show information that can have empty information or important data in them.  \n",
    "This depends in the group of `diaSrc` that each `diaObject` is associating. The column which knows about how many observations this is grouping is `nobs`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs.nobs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(objs.nobs, bins=range(50), log=True, histtype='step', lw=2, color='k')\n",
    "plt.xlabel('Number of DIASources in DIAObject')\n",
    "plt.ylabel('# DIAObjects / bin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some of them which actually have only 1 `diaSrc`, i.e. no association of sources at all.\n",
    "\n",
    "If we want to know which sources are associated to each `diaObject` we need the table called `diaObjectId`, which we ask for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objid = diabutler.get('deepDiff_diaObjectId', dataId=tpId).toDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the line above fails, might be due to different versions of the stack. The `diaObjectId` is a relatively new feature, and its creation is possible using the latest version of `dia_pipe`.\n",
    "\n",
    "An alternative way of accessing the table is through the parquet tables, where this information comes from. This files are located in the associated piece of the repo directory tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/rerun/assoc_sha/deepDiff/diaObject/4432/4,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objId = pd.read_parquet('/global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/rerun/assoc_sha/deepDiff/diaObject/4432/4,3/diaObjectId-4432-4,3.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objId['nobs'] = objId.diaSrcIds.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell from the first rows, the table only hosts two columns and the `diaSrcIds` contains a list of the `diaSrcs` that are linked to a particular `diaObject`.\n",
    "We also added the `nobs` column which is important to have it here too.\n",
    "Let's work with the subset of `diaObject` with more sources associated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_objs = objId[objId.nobs>=30]\n",
    "subset_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_row = 0\n",
    "print('the ID is : ', int(subset_objs.iloc[i_row]['diaObjectId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place it in our coordinate box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_obj = objs[objs['id'] == int(subset_objs.iloc[i_row]['diaObjectId'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(np.rad2deg(srcs['coord_ra']), np.rad2deg(srcs['coord_dec']), \n",
    "            c=np.log10(srcs['ip_diffim_forced_PsfFlux_instFlux']))\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.scatter(np.rad2deg(selected_obj['coord_ra']), \n",
    "            np.rad2deg(selected_obj['coord_dec']),\n",
    "            c='red', s=100, alpha=0.5)\n",
    "plt.grid()\n",
    "plt.vlines(x=[max_ra, min_ra], ymin=min_dec, ymax=max_dec)\n",
    "plt.hlines(y=[max_dec, min_dec], xmin=min_ra, xmax=max_ra)\n",
    "plt.gca().set_aspect(1./np.cos(np.mean([max_dec, min_dec])))\n",
    "plt.gca().invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dot shows which object we have chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select the data and start working to bring the `diaSrc` data altogether.  \n",
    "So the list of `diaSrcs` associated is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objdrow = objId[objId.diaObjectId==int(selected_obj['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcids = objdrow['diaSrcIds'].tolist()[0]\n",
    "srcids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(srcids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the individual detections we must know the visit, and detector where each `diaSrc` was found. For this we need to use an inverse generation Id trick, which for Run2.1i (and **only** Run2.1) is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_src(src_id):\n",
    "    visit_id = (asrcid>>26)//1000\n",
    "    det_id = (asrcid>>26)%(1000*visit_id)\n",
    "    return visit_id, det_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = []\n",
    "detectors = []\n",
    "for asrcid in srcids:\n",
    "    visit_id, det_id = get_id_from_src(asrcid)\n",
    "    \n",
    "    visits.append(visit_id)\n",
    "    detectors.append(det_id)\n",
    "\n",
    "srcidstab = pd.DataFrame(np.array([srcids, visits, detectors]).T, columns=['srcid', 'visit', 'det'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we would like to perform some sort of calibration for the individual fluxes. \n",
    "\n",
    "Below we are going to pull the `deepDiff_diaSrc` table where our source id is pointing to and also the `calexp` calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes_to_calibrate = ['base_PsfFlux_instFlux',\n",
    "                       'ip_diffim_forced_PsfFlux_instFlux',\n",
    "                       'base_CircularApertureFlux_3_0_instFlux',\n",
    "                       'base_CircularApertureFlux_4_5_instFlux',\n",
    "                       'base_CircularApertureFlux_6_0_instFlux',\n",
    "                       'base_CircularApertureFlux_9_0_instFlux',\n",
    "                       'base_CircularApertureFlux_12_0_instFlux', \n",
    "                       'base_CircularApertureFlux_17_0_instFlux', \n",
    "                       'base_CircularApertureFlux_25_0_instFlux']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct (DM-way) of correcting the flux and magnitude of a given source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibrated_values(fluxname, source_row, photcal):\n",
    "    \"\"\"\n",
    "    Obtain the calibration using the DM tools for that particular image\n",
    "    \n",
    "    This way of converting fluxes to magnitudes is aware of the zero points\n",
    "    of the image as well as the correct calculation.\n",
    "    \n",
    "    We can obtain then, magnitudes and fluxes in NJy\n",
    "    \"\"\"\n",
    "    flux, err = source_row[aflux], source_row[aflux+'Err'] \n",
    "    #print(flux, err)\n",
    "    cal_mag = photcal.instFluxToMagnitude(flux, err)\n",
    "    cal_flux = photcal.instFluxToNanojansky(flux, err)\n",
    "    return cal_mag, cal_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for idx, anid, avisit, adet in srcidstab.itertuples():\n",
    "    # Here we just ask for the sources tab and the \n",
    "    # calibration for the image which they come from\n",
    "    sources_tab = diabutler.get('deepDiff_diaSrc', \n",
    "                                visit=avisit, detector=adet).asAstropy().to_pandas()\n",
    "    # get the calibration tool\n",
    "    photcal = diabutler.get('deepDiff_differenceExp_photoCalib', \n",
    "                           visit=avisit, detector=adet)\n",
    "    \n",
    "    # get the calexp to obtain MJD and filter\n",
    "    calexp = diabutler.get('calexp', visit=avisit, detector=adet)\n",
    "    date_mjd = calexp.getInfo().getVisitInfo().getDate().get()\n",
    "    fltr = calexp.getInfo().getFilter().getCanonicalName()\n",
    "    \n",
    "    # include this in the source table \n",
    "    sources_tab['filter'] = fltr\n",
    "    sources_tab['MJD'] = date_mjd\n",
    "    \n",
    "    source_row = sources_tab[sources_tab.id == anid].copy()\n",
    "    \n",
    "    for aflux in fluxes_to_calibrate:\n",
    "        cal_mag, cal_flux = get_calibrated_values(aflux, source_row, photcal)\n",
    "\n",
    "        source_row[aflux+'_calMag'] = cal_mag.value\n",
    "        source_row[aflux+'_calMagErr'] = cal_mag.error\n",
    "        source_row[aflux+'_nJy'] = cal_flux.value\n",
    "        source_row[aflux+'_nJyErr'] = cal_flux.error\n",
    "\n",
    "    rows.append(source_row)\n",
    "lightcurve = pd.concat(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a lightcurve table ready to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fltr, lc in lightcurve.groupby('filter'):\n",
    "    plt.errorbar(lc['MJD'], lc['base_CircularApertureFlux_6_0_instFlux_calMag'], \n",
    "                 yerr=lc['base_CircularApertureFlux_6_0_instFlux_calMagErr'],\n",
    "                 label=fltr, fmt='o')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('MJD')\n",
    "plt.ylabel('Circular Ap 6.0 mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the techniques in [this Notebook](dia_source_object_stamp.ipynb) to create stamps, but for the moment this is enough information!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-dia",
   "language": "python",
   "name": "desc-dia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
