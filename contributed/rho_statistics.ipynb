{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing star shapes, shape correlation functions, and rho statistics\n",
    "======================================================================\n",
    "\n",
    "This code demonstrates how to measure star and PSF shapes from the object catalogs and how to compute shape-shape correlation functions.  It has a few steps:\n",
    " - Load a star catalog and a galaxy catalog from the imaging area\n",
    " - Load the extragalactic catalog and truth catalog over the same area\n",
    " - Match the imaging and EGC catalogs \n",
    " - Measure the galaxy-galaxy shear correlation functions of the two galaxy catalogs\n",
    " - Measure the \"rho statistics\" -- correlation functions of the PSF and star shapes that diagnose PSF leakage into the cosmic shear signal\n",
    " - Measure calibration factors that allow us to compare measured shapes to intrinsic shapes\n",
    " - Correct the galaxy-galaxy shear CF from the imaging data and compare the corrected version to the EGC CF\n",
    " - Measure the correlation function of the EGC over the same area to further look for selection bias\n",
    " \n",
    "Many of these steps borrow heavily from other DC2 tutorials, as listed in the code comments.\n",
    "\n",
    "Note that--because we are loading many catalogs and performing a lot of processing on them that may not be vectorized--this notebook can take longer than you might expect to run, a few minutes per tract per catalog just to load the catalogs.\n",
    "\n",
    "We begin by loading up the code we'll need. The desc-stack-dev kernel should have the dependencies you need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "# Allow us to import \"utils.fieldRotator\" below\n",
    "sys.path.insert(0, '../tutorials')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import FoFCatalogMatching\n",
    "import lsst.afw.geom as afw_geom\n",
    "import lsst.daf.persistence as dp\n",
    "import GCRCatalogs\n",
    "from GCR import GCRQuery\n",
    "from desc_dc2_dm_data import REPOS\n",
    "from utils.fieldRotator import FieldRotator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as op\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # Stile throws up a bunch of matplotlib warnings that we can just ignore\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import stile\n",
    "\n",
    "figsize_x, figsize_y = plt.gcf().get_size_inches()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting star shapes\n",
    "-------------------\n",
    "\n",
    "This next object demonstrates how to compute star shapes from the object catalogs.  You will need the measured _moments_: Ixx, Ixy, and Iyy.  These are measured in a reference frame that is local to the tracts and patches that the DM stack uses for analysis.  To compare to the galaxy shapes (which are in sky coordinates, relative to the (ra, dec) direction), we also need to rotate to the sky coordinate system using the butler.  \n",
    "\n",
    "To perform this, we'll set up a ShearRotator class that takes in the moment data and reproduces the quantities we want.  Note that, per the notation sometimes used by lensing people (and, in particular, used by Stile), \"sigma\" is a linear measurement of size, and not a measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShearRotator:\n",
    "    \"\"\" A class to compute distortions from moments, and optionally to rotate the results.  The outputs\n",
    "        of the DM stack are in the local coordinate frame used for coaddition, but we want sky coordinates\n",
    "        relative to ra and dec).  This class will store the info needed to rotate the moments (Ixx, Iyy, \n",
    "        Ixy, or IxxPSf, IyyPSF, IxyPSF) and additionally compute errors, if the covariance matrices are \n",
    "        also provided (Cxx etc).  Each desired quantity has a different function (g1, g2, sigma, and their\n",
    "        errors) but heavy computations are stored to facilitate faster returns.\n",
    "        \n",
    "        If you want to use a ShearRotator for multiple catalogs (that is, multiple positions),\n",
    "        then you MUST call ShearRotator.clear() between catalogs to remove the saved versions of the \n",
    "        rotated moments.\n",
    "        \n",
    "        Note that sigma is given in units of arcsec^2!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        butler: lsst.daf.persistence.Butler\n",
    "            A butler from which to retrieve the calibrated exposures and WCS\n",
    "        tractId: int\n",
    "            The number of the desired tract\n",
    "        ra: array_like\n",
    "            An array of right ascensions\n",
    "        dec: array_like\n",
    "            An array of declinations\n",
    "        Ixx: array_like\n",
    "            An array of Ixx (the (0,0) component of a moments matrix)\n",
    "        Iyy: array_like\n",
    "            An array of Iyy (the (1,1) component of a moments matrix)\n",
    "        Ixy: array_like\n",
    "            An array of Ixy (the (0,1) component of a moments matrix)\n",
    "        Cxx: array_like\n",
    "            An array of Cxx (the (0,0) component of a moments covariance matrix). Can be None\n",
    "            if no errors will be computed.\n",
    "        Cyy: array_like\n",
    "            An array of Cyy (the (1,1) component of a moments covariance matrix). Can be None\n",
    "            if no errors will be computed.\n",
    "        Cxy: array_like\n",
    "            An array of Cxy (the (0,1) component of a moments covariance matrix). Can be None\n",
    "            if no errors will be computed.\n",
    "        convert_to_sky_coords: bool\n",
    "            Whether to rotate to sky coordinates or leave in native coordinates (default: True)\n",
    "       \"\"\"\n",
    "    def __init__(self, butler, tractId, ra, dec, Ixx, Iyy, Ixy, Cxx=None, Cyy=None, Cxy=None, \n",
    "                       convert_to_sky_coords=True):\n",
    "            \n",
    "        self.ra = ra\n",
    "        self.dec = dec\n",
    "        if convert_to_sky_coords:\n",
    "            skymap = butler.get('deepCoadd_skyMap')\n",
    "            self.wcs = skymap[tractId].getWcs()\n",
    "            self.makeTransform()\n",
    "        else:\n",
    "            self.wcs = None\n",
    "        \n",
    "        self.Ixx, self.Iyy, self.Ixy = self.makeRotatedMoments(Ixx, Iyy, Ixy)\n",
    "        self.Cxx, self.Cyy, self.Cxy = self.makeRotatedCovariance(Cxx, Cyy, Cxy)\n",
    "        \n",
    "    def makeTransform(self):\n",
    "        centroids = [afw_geom.Point2D(tra, tdec) for tra, tdec in zip(self.ra, self.dec)]\n",
    "        self.transforms = [self.wcs.linearizePixelToSky(centroid, afw_geom.degrees).getLinear()\n",
    "                                           for centroid in centroids]\n",
    "\n",
    "    def makeRotatedMoments(self, Ixx, Iyy, Ixy):\n",
    "        if self.wcs:\n",
    "            ellipses = [afw_geom.ellipses.Quadrupole(tixx, tiyy, tixy) \n",
    "                        for tixx, tiyy, tixy in zip(Ixx, Iyy, Ixy)]\n",
    "            moments = [ellipse.transform(lt) for ellipse, lt in zip(ellipses, self.transforms)]\n",
    "            Ixx = np.array([mom.getIxx() for mom in moments])\n",
    "            Ixy = np.array([mom.getIxy() for mom in moments])\n",
    "            Iyy = np.array([mom.getIyy() for mom in moments])\n",
    "        return Ixx, Iyy, Ixy\n",
    "    \n",
    "    def makeRotatedCovariance(self, Cxx, Cyy, Cxy):\n",
    "        if self.wcs and Cxx is not None:\n",
    "            cov_ixx = np.zeros(Cxx.shape)\n",
    "            cov_iyy = np.zeros(Cxx.shape)\n",
    "            cov_ixy = np.zeros(Cxx.shape)\n",
    "            for i, (tcxx, tcxy, tcyy, lt) in enumerate(zip(Cxx, Cxy, Cyy, self.transforms)):\n",
    "                cov_ixx[i] = (lt[0,0]**4*tcxx +\n",
    "                              (2.*lt[0,0]*lt[0,1])**2*tcyy + lt[0,1]**4*tcxy)\n",
    "                cov_iyy[i] = (lt[1,0]**4*tcxx +\n",
    "                              (2.*lt[1,0]*lt[1,1])**2*tcyy + lt[1,1]**4*tcxy)\n",
    "                cov_ixy[i] = ((lt[0,0]*lt[1,0])**2*tcxx +\n",
    "                              (lt[0,0]*lt[1,1]+lt[0,1]*lt[1,0])**2*tcyy +\n",
    "                              (lt[0,1]*lt[1,1])**2*tcxy)\n",
    "            return cov_ixx, cov_iyy, cov_ixy\n",
    "        return Cxx, Cyy, Cxy\n",
    "    \n",
    "    def g1(self):\n",
    "        return (self.Ixx-self.Iyy)/(self.Ixx+self.Iyy)\n",
    "\n",
    "    def g2(self):\n",
    "        return 2.*self.Ixy/(self.Ixx+self.Iyy)\n",
    "    \n",
    "    def sigma(self):\n",
    "        return 3600*(self.Ixx*self.Iyy - self.Ixy**2)**0.25 #3600: in arcsec\n",
    "    \n",
    "    def g1_err(self):\n",
    "        dg1_dixx = 2.*self.Iyy/(self.Ixx+self.Iyy)**2\n",
    "        dg1_diyy = -2.*self.Ixx/(self.Ixx+self.Iyy)**2\n",
    "        return np.sqrt(dg1_dixx**2 * self.Cxx + dg1_diyy**2 * self.Cyy)\n",
    "\n",
    "    def g2_err(self):\n",
    "        dg2_dixx = -2.*self.Ixy/(self.Ixx+self.Iyy)**2\n",
    "        dg2_diyy = -2.*self.Ixy/(self.Ixx+self.Iyy)**2\n",
    "        dg2_dixy = 2./(self.Ixx+self.Iyy)\n",
    "        return np.sqrt(dg2_dixx**2 * self.Cxx + dg2_diyy**2 * self.Cyy +\n",
    "                            dg2_dixy**2 * self.Cxy)\n",
    "\n",
    "    def sigma_err(self):\n",
    "        sigma = self.sigma()\n",
    "        dsigma_dixx = 0.25/sigma**3*self.Iyy\n",
    "        dsigma_diyy = 0.25/sigma**3*self.Ixx\n",
    "        dsigma_dixy = -0.5/sigma**3*self.Ixy\n",
    "        return 3600*np.sqrt(dsigma_dixx**2 * self.Cxx + dsigma_diyy**2 * self.Cyy +\n",
    "                               dsigma_dixy**2 * self.Cxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing catalogs\n",
    "-----------------\n",
    "\n",
    "Next, let's get catalogs to do some analysis. We'll want four different catalogs:\n",
    "1. The extragalactic catalog\n",
    "2. The truth catalog (for matching positions)\n",
    "3. An object catalog (from DM processing) with galaxy shapes and PSF shapes at the locations of the galaxies\n",
    "4. An object catalog with star shapes and PSF shapes at the locations of the stars\n",
    "\n",
    "We'll select these catalogs on a tract-by-tract basis.  By default, we will use tract 4851; for the run 1.2p that we are using, we could use tracts 4429-4433, 4636-4640, 4848-4852, or 5062-5066 (or a list of any or all of those--note that the runtime of this notebook goes up with the number of tracts, though).  To limit the catalogs to the correct area, we use code from [this tutorial by Jim Chiang](https://nbviewer.jupyter.org/github/LSSTDESC/DC2-analysis/blob/rendered/tutorials/matching_stack.nbconvert.ipynb).  To do the shear rotation defined above, and the tract selection here, we will use a DM stack butler, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial code to select a given region. We have altered the __call__ method of this code to give us a mask only, \n",
    "# rather than generating a catalog from the subset.\n",
    "class RegionSelector:\n",
    "    \"\"\"\n",
    "    Class to rotate the protoDC2 galaxies to the Run1.1p sky location and downselect those galaxies\n",
    "    based on a magnitude limit and on the coordinates of the subregion (i.e., patch or CCD) being\n",
    "    considered.\n",
    "    \"\"\"\n",
    "    protoDC2_ra = 55.064\n",
    "    protoDC2_dec = -29.783\n",
    "    field_rotator = FieldRotator(0, 0, protoDC2_ra, protoDC2_dec)\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _set_coord_range(self, bbox, wcs):\n",
    "        \"\"\"\n",
    "        Set the coordinate range of the region.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        This method is used by the RegionSelector's subclasses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        bbox: Calexp.BBox\n",
    "            Defines corners of region's bounding box\n",
    "        wcs: Calexp.Wcs\n",
    "            Defines pixel to world (sky) coordinate transformation\n",
    "        \"\"\"\n",
    "        region_box = afw_geom.Box2D(bbox)\n",
    "        corners = region_box.getCorners()\n",
    "        ra_values, dec_values = [], []\n",
    "        for corner in corners:\n",
    "            ra, dec = wcs.pixelToSky(corner)\n",
    "            ra_values.append(ra.asDegrees())\n",
    "            dec_values.append(dec.asDegrees())\n",
    "        self.ra_range = min(ra_values), max(ra_values)\n",
    "        self.dec_range = min(dec_values), max(dec_values)\n",
    "        \n",
    "    def __call__(self, gc):\n",
    "        \"\"\"\n",
    "        Return a Boolean mask indicating which items from the given catalog are in the region defined by this object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gc: dict\n",
    "            A catalog of objects (such as galaxies or stars) in dict form, including the keys 'ra_true' and 'dec_true'\n",
    "            or 'ra' and 'dec'. Ideally from a get_quantities() call from a GCRCatalog, but that's not required.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask: a boolean mask of the same length as gc['ra_true'] and gc['dec_true'] (or 'ra' and 'dec)')\n",
    "        \"\"\"\n",
    "        # Rotate to the Run1.2 field if an EGC; otherwise don't.\n",
    "        if 'ra_true' in gc:\n",
    "            gc_ra_rot, gc_dec_rot \\\n",
    "                = self.field_rotator.transform(gc['ra_true'],\n",
    "                                               gc['dec_true'])\n",
    "        else:\n",
    "            gc_ra_rot, gc_dec_rot = gc['ra'], gc['dec']\n",
    "\n",
    "        # Select the galaxies within the region.\n",
    "        mask = ((gc_ra_rot > self.ra_range[0]) &\n",
    "                 (gc_ra_rot < self.ra_range[1]) &\n",
    "                 (gc_dec_rot > self.dec_range[0]) &\n",
    "                 (gc_dec_rot < self.dec_range[1]))\n",
    "        # Return mask, and handle it in the calling function\n",
    "        return mask\n",
    "\n",
    "class PatchSelector(RegionSelector):\n",
    "    \"\"\"RegionSelector to use with skyMap patches, i.e., coadd data.\"\"\"\n",
    "    def __init__(self, butler, tract, patch):\n",
    "        super(PatchSelector, self).__init__()\n",
    "        # Get the patch boundaries.\n",
    "        skymap = butler.get('deepCoadd_skyMap')\n",
    "        tractInfo = skymap[tract]\n",
    "        patchInfo = tractInfo.getPatchInfo(eval(patch))\n",
    "        self._set_coord_range(patchInfo.getOuterBBox(), tractInfo.getWcs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_butler(repo):\n",
    "    # Create a data butler for the given repo.\n",
    "    butler = dp.Butler(repo)\n",
    "    return butler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a butler and get a skymap object. This will let us iterate over tracts and patches.\n",
    "# We'll be using run 1.2p throughout this notebook.\n",
    "butler = get_butler(REPOS['1.2p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the EGC in the tract area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catalog(catalog='proto-dc2_v3.0',\n",
    "                butler=None, tract=4851, \n",
    "                mag_max=24.5, mag_col='mag_true_i_lsst',\n",
    "                quantities = ['galaxy_id', 'ra_true', 'dec_true', \n",
    "                              'size_true', 'ellipticity_1_true', 'ellipticity_2_true',\n",
    "                              'shear_1', 'shear_2_treecorr']):\n",
    "    \"\"\" Get a catalog with the given quantities from the given tract(s), with magnitudes of the given kind less than mag_max.\n",
    "        This function is designed for EGC or truth catalogs, whose columns don't need any extra processing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        catalog: str\n",
    "            The name of a catalog known to GCRCatalogs\n",
    "        butler: lsst.daf.persistence.Butler object\n",
    "            A butler containing information about the tracts and patches you would like to extract.\n",
    "        tract: int or iterable of ints\n",
    "            The tract(s) to extract\n",
    "        mag_max: float\n",
    "            The maximum magnitude of returned objects\n",
    "        mag_col: str\n",
    "            The column of magnitudes that should be less than mag_max\n",
    "        quantities: iterable of str\n",
    "            The desired columns from the catalog to return\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        return_catalog: dict\n",
    "            A dictionary containing the columns defined by `quantities` in the region of the chosen tract.\n",
    "        \"\"\"\n",
    "    skymap = butler.get('deepCoadd_skyMap')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        cat = GCRCatalogs.load_catalog(catalog)\n",
    "\n",
    "    cuts = ['{} < {}'.format(mag_col, mag_max)]\n",
    "    catalog = cat.get_quantities(quantities, filters=cuts)\n",
    "    \n",
    "    # We will loop over the tracts and their patches and get the mask for this region of sky.\n",
    "    # We don't just get a subset of the catalog--since the tracts and patches may not be perfect \n",
    "    # rectangles, doing multiple patches/tracts might lead to duplicate entries.  Instead, we'll\n",
    "    # simply generate a bunch of masks, \"or\" them, and apply them all at once before returning.\n",
    "    masks = []\n",
    "    if not hasattr(tract, '__iter__'):\n",
    "        tract = [tract]\n",
    "    for this_tract in tract:\n",
    "        for patch in skymap[this_tract]:\n",
    "            patchId = '%d,%d' % patch.getIndex()\n",
    "            region_selector = PatchSelector(butler, this_tract, patchId)\n",
    "            mask = region_selector(catalog)\n",
    "            masks.append(mask)\n",
    "    # This line does a multi-array \"or\" all at once, in a memory-expensive kind of way.\n",
    "    mask = np.any(np.array(masks), axis=0)\n",
    "    # The catalog object is a dict, so we need to mask each column individually.\n",
    "    return {k: catalog[k][mask] for k in catalog}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract = 4851\n",
    "egc_catalog = get_catalog('proto-dc2_v3.0', butler, tract) # defaults are okay for this purpose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a truth catalog.  This is nearly the same except our quantities are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_catalog = get_catalog('dc2_truth_run1.2_static', butler, tract,\n",
    "                            quantities=['ra', 'dec', 'object_id', 'star', 'sprinkled'],\n",
    "                            mag_col='mag_true_i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a galaxy catalog from the object table (the results of the DM run).  This requires a little more massaging: we want the PSF shapes (which requires rotating shears, as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_galaxy_catalog(object_catalog='dc2_object_run1.2p_all_columns',\n",
    "                       butler=None, tract=4851, \n",
    "                       mag_max=24.5, filter_='i'):\n",
    "    \"\"\" Get an object catalog containing galaxies from the given tract(s), with magnitudes in the given filter\n",
    "        less than mag_max.  This function works only on object catalogs, and has less freedom than the \n",
    "        get_catalog() method above, since it requires significantly more processing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        catalog: str\n",
    "            The name of a catalog known to GCRCatalogs.  This should be an object catalog with the\n",
    "            \"all_columns\" suffix to make sure it has the required moments columns.\n",
    "        butler: lsst.daf.persistence.Butler object\n",
    "            A butler containing information about the tracts and patches you would like to extract.\n",
    "        tract: int or iterable of ints\n",
    "            The tract(s) to extract\n",
    "        mag_max: float\n",
    "            The maximum magnitude of returned objects\n",
    "        filter_: str\n",
    "            The filter to check for magnitudes and SNR.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        return_catalog: dict\n",
    "            A dictionary containing galaxy shape and PSF shape columns in the given tract(s), plus\n",
    "            positions.\n",
    "        \"\"\"\n",
    "    catalog = GCRCatalogs.load_catalog(object_catalog)\n",
    "    catalog.add_quantity_modifier('shape_hsm_regauss_etot', \n",
    "                                  (np.hypot, 'ext_shapeHSM_HsmShapeRegauss_e1', 'ext_shapeHSM_HsmShapeRegauss_e2'), \n",
    "                                  overwrite=True)\n",
    "    \n",
    "    # Galaxy cuts are from the tutorial by F. Lanusse and J. Sanchez referenced at the top of this file\n",
    "    galaxy_cuts = [\n",
    "        GCRQuery('extendedness > 0'),     # Extended objects\n",
    "        GCRQuery((np.isfinite, 'mag_i')), # Select objects that have i-band magnitudes\n",
    "        GCRQuery('clean'), # The source has no flagged pixels (interpolated, saturated, edge, clipped...) \n",
    "                           # and was not skipped by the deblender\n",
    "        GCRQuery('xy_flag == 0'),                                      # Flag for bad centroid measurement\n",
    "        GCRQuery('ext_shapeHSM_HsmShapeRegauss_flag == 0'),            # Error code returned by shape measurement code\n",
    "        GCRQuery((np.isfinite, 'ext_shapeHSM_HsmShapeRegauss_sigma')), # Shape measurement uncertainty should not be NaN\n",
    "        GCRQuery('snr_{}_cModel > 10'.format(filter_)),                              # SNR > 10\n",
    "        GCRQuery('mag_{}_cModel < {}'.format(filter_, mag_max)),                     # cModel imag brighter than 24.5\n",
    "        GCRQuery('ext_shapeHSM_HsmShapeRegauss_resolution >= 0.3'), # Sufficiently resolved galaxies compared to PSF\n",
    "        GCRQuery('shape_hsm_regauss_etot < 2'),                     # Total distortion in reasonable range\n",
    "        GCRQuery('ext_shapeHSM_HsmShapeRegauss_sigma <= 0.4'),      # Shape measurement errors reasonable\n",
    "    ]    \n",
    "\n",
    "    \n",
    "    # HsmShapeRegauss is the current implementation of the shape measurement algorithm.  It's already corrected for PSF shapes,\n",
    "    # so it is the shape catalog we want to use.  The \"resolution\" tells you how big the galaxy is relative to the PSF size,\n",
    "    # which we'll use for diagnostics later.\n",
    "    # We also get the moments measurements for the PSF: those are the InnPSF quantities.\n",
    "    galaxy_quantities = ['ra', 'dec', \n",
    "                         'ext_shapeHSM_HsmShapeRegauss_e1', 'ext_shapeHSM_HsmShapeRegauss_e2', 'ext_shapeHSM_HsmShapeRegauss_sigma',\n",
    "                         'IxxPSF', 'IxyPSF', 'IyyPSF', 'ext_shapeHSM_HsmShapeRegauss_resolution']\n",
    "    if not hasattr(tract, '__iter__'):\n",
    "        tract = [tract]\n",
    "\n",
    "    # Because the object catalog takes so long to load, and because we're using a \"native filter\" of tract number,\n",
    "    # we can and will load in only the part of the catalog in the requested tracts.  Also, since we're doing native\n",
    "    # filters, we don't need to do the area masking.  And since the skymaps are defined on a per-tract basis, we \n",
    "    # don't need to bother looping through patches!\n",
    "    list_of_galaxy_catalogs = []\n",
    "    for this_tract in tract:\n",
    "        tract_filter = 'tract == {}'.format(this_tract)\n",
    "        galaxy_catalog = catalog.get_quantities(galaxy_quantities, \n",
    "                                                filters=galaxy_cuts,\n",
    "                                                native_filters = tract_filter)\n",
    "        shear_rotator = ShearRotator(butler, this_tract, \n",
    "                                     galaxy_catalog['ra'], galaxy_catalog['dec'],\n",
    "                                     galaxy_catalog['IxxPSF'], galaxy_catalog['IyyPSF'], galaxy_catalog['IxyPSF'])\n",
    "        galaxy_catalog['psf_e1'] = shear_rotator.g1()\n",
    "        galaxy_catalog['psf_e2'] = shear_rotator.g2()\n",
    "        # Remember, \"sigma\" is the psf SHAPE not the psf measurement error!\n",
    "        galaxy_catalog['psf_sigma'] = shear_rotator.sigma()\n",
    "        list_of_galaxy_catalogs.append(galaxy_catalog)\n",
    "    # Now, loop over the list of galaxy catalogs we made, and return a dict where each key is the concatenation\n",
    "    # of the values in the different galaxy catalogs.\n",
    "    return {k: np.concatenate([l[k] for l in list_of_galaxy_catalogs]) for k in list_of_galaxy_catalogs[0]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_catalog = get_galaxy_catalog(butler=butler, tract=tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for star data, except we will compute the star shapes directly from the moments (we don't WANT the PSF-corrected star shapes: the star shapes ARE the PSF shapes!), and we use different cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_star_catalog(object_catalog='dc2_object_run1.2p_all_columns',\n",
    "                     butler=None, tract=4851, \n",
    "                     mag_max=24.5, filter_='i'):\n",
    "    \"\"\" Get an object catalog containing stars from the given tract(s), with magnitudes in the given filter less \n",
    "        than mag_max.  This function works only on object catalogs, and has less freedom than the get_catalog() \n",
    "        method above, since it requires significantly more processing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        catalog: str\n",
    "            The name of a catalog known to GCRCatalogs.  This should be an object catalog with the\n",
    "            \"all_columns\" suffix to make sure it has the required moments columns.\n",
    "        butler: lsst.daf.persistence.Butler object\n",
    "            A butler containing information about the tracts and patches you would like to extract.\n",
    "        tract: int or iterable of ints\n",
    "            The tract(s) to extract\n",
    "        mag_max: float\n",
    "            The maximum magnitude of returned objects\n",
    "        filter_: str\n",
    "            The filter to check for magnitudes and SNR.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        return_catalog: dict\n",
    "            A dictionary containing star shape and PSF shape columns in the given tract(s), plus\n",
    "            positions.\n",
    "        \"\"\"\n",
    "    catalog = GCRCatalogs.load_catalog(object_catalog)\n",
    "    \n",
    "    # Galaxy cuts are from the tutorial by F. Lanusse and J. Sanchez referenced at the top of this file\n",
    "    star_cuts = [\n",
    "        GCRQuery('extendedness == 0'),                            # Not extended objects!\n",
    "        GCRQuery('mag_{}_cModel < {}'.format(filter_, mag_max)),  # Above flux limit (probably will be for stars)\n",
    "        GCRQuery('mag_{}_cModel > 0'.format(filter_))             # A reasonable mag measurement\n",
    "    ]\n",
    "\n",
    "    \n",
    "    star_quantities = ['ra', 'dec',\n",
    "                       'Ixx', 'Ixy', 'Iyy',\n",
    "                       'IxxPSF', 'IxyPSF', 'IyyPSF']\n",
    "    if not hasattr(tract, '__iter__'):\n",
    "        tract = [tract]\n",
    "\n",
    "    # Because the object catalog takes so long to load, and because we're using a \"native filter\" of tract number,\n",
    "    # we can and will load in only the part of the catalog in the requested tracts.  Also, since we're doing native\n",
    "    # filters, we don't need to do the area masking.  And since the skymaps are defined on a per-tract basis, we \n",
    "    # don't need to bother looping through patches!\n",
    "    list_of_star_catalogs = []\n",
    "    for this_tract in tract:\n",
    "        tract_filter = 'tract == {}'.format(this_tract)\n",
    "        star_catalog = catalog.get_quantities(star_quantities, \n",
    "                                              filters=star_cuts,\n",
    "                                              native_filters = tract_filter)\n",
    "        shear_rotator = ShearRotator(butler, this_tract, \n",
    "                                     star_catalog['ra'], star_catalog['dec'],\n",
    "                                     star_catalog['Ixx'], star_catalog['Iyy'], star_catalog['Ixy'])\n",
    "        star_catalog['e1'] = shear_rotator.g1()\n",
    "        star_catalog['e2'] = shear_rotator.g2()\n",
    "        # Remember, \"sigma\" is the psf SHAPE not the psf measurement error!\n",
    "        star_catalog['sigma'] = shear_rotator.sigma()\n",
    "\n",
    "        shear_rotator = ShearRotator(butler, this_tract, \n",
    "                                     star_catalog['ra'], star_catalog['dec'],\n",
    "                                     star_catalog['IxxPSF'], star_catalog['IyyPSF'], star_catalog['IxyPSF'])\n",
    "        star_catalog['psf_e1'] = shear_rotator.g1()\n",
    "        star_catalog['psf_e2'] = shear_rotator.g2()\n",
    "        star_catalog['psf_sigma'] = shear_rotator.sigma()\n",
    "        list_of_star_catalogs.append(star_catalog)\n",
    "    # Now, loop over the list of star catalogs we made, and return a dict where each key is the concatenation\n",
    "    # of the values in the different star catalogs.\n",
    "    return {k: np.concatenate([l[k] for l in list_of_star_catalogs]) for k in list_of_star_catalogs[0]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_catalog = get_star_catalog(butler=butler, tract=tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the catalogs together\n",
    "---------------------------\n",
    "\n",
    "Following the [FoF matching tutorial](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/matching_fof.ipynb), we'll match the truth catalog and galaxy catalogs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the truth catalog to the DM catalog, and the EGC to the truth catalog\n",
    "\n",
    "matches = FoFCatalogMatching.match(\n",
    "    catalog_dict={'truth': truth_catalog, 'object': galaxy_catalog},\n",
    "    linking_lengths=1.0,\n",
    "    catalog_len_getter=lambda x: len(x['ra']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, find the one-to-one matches.\n",
    "# We could probably be more clever and try to figure out what's going on with the non-matches,\n",
    "# because this probably has a weird selection, but it should be good enough for now.\n",
    "truth_mask = matches['catalog_key'] == 'truth'\n",
    "object_mask = ~truth_mask\n",
    "\n",
    "n_groups = matches['group_id'].max() + 1\n",
    "n_truth = np.bincount(matches['group_id'][truth_mask], minlength=n_groups)\n",
    "n_object = np.bincount(matches['group_id'][object_mask], minlength=n_groups)\n",
    "\n",
    "one_to_one_group_mask = np.in1d(matches['group_id'], np.flatnonzero((n_truth == 1) & (n_object == 1)))\n",
    "truth_idx = matches['row_index'][one_to_one_group_mask & truth_mask]\n",
    "object_idx = matches['row_index'][one_to_one_group_mask & object_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pandas DataFrame that merges all the catalogs and discards non-matches\n",
    "truth_table = pd.DataFrame(truth_catalog).iloc[truth_idx].reset_index(drop=True)\n",
    "object_table = pd.DataFrame(galaxy_catalog).iloc[object_idx].reset_index(drop=True)\n",
    "merged_table = pd.merge(truth_table, object_table, left_index=True, right_index=True, suffixes=('_truth', '_object'))\n",
    "merged_table = pd.merge(merged_table, pd.DataFrame(egc_catalog), 'inner', left_on='object_id', right_on='galaxy_id', suffixes=('', '_egc'))\n",
    "# some EGC things are named \"_true\" and having both \"_true\" and \"_truth\" is confusing! Rename those.\n",
    "merged_table = merged_table.rename(columns=lambda x: x[:-5]+'_egc' if x[-5:] == '_true' else x) \n",
    "# Make some column names easier to type\n",
    "merged_table = merged_table.rename(columns={\"ext_shapeHSM_HsmShapeRegauss_e1\" : \"e1_object\", \n",
    "                                            \"ext_shapeHSM_HsmShapeRegauss_e2\" : \"e2_object\",\n",
    "                                            \"ellipticity_1_egc\": \"e1_egc\",\n",
    "                                            \"ellipticity_2_egc\": \"e2_egc\"})\n",
    "    \n",
    "print(\"Number of matches: {} from {} DM galaxies and {} truth galaxies\".format(\n",
    "            len(merged_table['ra_truth']), len(galaxy_catalog['ra']), len(truth_catalog['ra'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will measure the calibration factors we need to go from the measured HSM shapes to the catalog shapes.  We usually use a linear decomposition: $m$ and $c$.  Most shape measurement codes have been found to be linear in the weak lensing regime ([Mandelbaum et al 2015](https://arxiv.org/abs/1412.1825)), so this should be sufficient for our purposes.  That means that for a given intrinsic shape $g^{\\rm true}$, what we actually measure is:\n",
    "\n",
    "$$ g^{\\rm meas} = (1+m)g^{\\rm true} + c $$\n",
    "\n",
    "The $c$ portion also has a dependence on the PSF ellipticity that we will parameterize through $\\alpha$, so:\n",
    "\n",
    "$$ g^{\\rm meas} = (1+m)g^{\\rm true} + \\alpha e^{\\rm PSF} + c $$\n",
    "\n",
    "The proper way to do this measurement is to measure the average biases from an ensemble of shears, since what we care about is the response of the ensemble to a coherent lensing distortion of some kind.  Here, we'll do something a little simpler: just fit a linear model in the form of the previous equation. This isn't exactly right, but it should be close enough to get the right order of magnitude for our purposes.  We will also assume there is no difference between the two components of the ellipticity (which should be close enough for this exercise) as we fit for the coefficients.  To help avoid confusion, we'll call these $k_1$, $k_2$ and $k_3$ instead of $m$, $\\alpha$ and $c$.\n",
    "\n",
    "We have one final factor to take care of: the HSM shapes measure a distortion, which is a different measure of ellipticity than the one used to define the catalog ellipticities.  They differ by a factor of 2 for a shear of 0, so we will apply that factor when doing the fitting.  (See [Bernstein & Jarvis 2002](https://arxiv.org/abs/astro-ph/0107431) for more detail on this difference.)  Again, this isn't quite right--galaxies aren't round--but it will be self-consistent with our measurement method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_model(x, k1, k2, k3):\n",
    "    xtrue = x[0]\n",
    "    xpsf = x[1]\n",
    "    return 2*((1+k1)*xtrue + k2*xpsf + k3)\n",
    "\n",
    "# The EGC and object catalogs seem to have different sign conventions\n",
    "e_egc_all = np.concatenate([merged_table['e1_egc'], merged_table['e2_egc']])\n",
    "psf_e_all = np.concatenate([-1*merged_table['psf_e1'], merged_table['psf_e2']])\n",
    "e_obj_all =  np.concatenate([-1*merged_table['e1_object'], merged_table['e2_object']])\n",
    "param, _ =  op.curve_fit(bias_model, [e_egc_all, psf_e_all], e_obj_all)\n",
    "\n",
    "k1 = param[0]\n",
    "k2 = param[1]\n",
    "k3 = param[2]\n",
    "\n",
    "print(\"m={}, alpha={}, c={}\".format(*param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_psf_e1 = merged_table['psf_e1'].mean()\n",
    "avg_psf_e2 = merged_table['psf_e2'].mean()\n",
    "\n",
    "# Plot what these biases actually look like\n",
    "fig = plt.figure(figsize=[2*figsize_x, figsize_y])\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist2d(merged_table['e1_egc'], -merged_table['e1_object'], bins=20)\n",
    "ax.plot(merged_table['e1_egc'], 2*(1+k1)*merged_table['e1_egc']+k2*avg_psf_e1+k3, color='black')\n",
    "ax.set_xlabel(\"EGC g1\")\n",
    "ax.set_ylabel(\"Object g1\")\n",
    "ax.set_ylim((-1, 1))\n",
    "ax = fig.add_subplot(122)\n",
    "ax.hist2d(merged_table['e2_egc'], merged_table['e2_object'], bins=20)\n",
    "ax.plot(merged_table['e2_egc'], 2*(1+k1)*merged_table['e2_egc']+k2*avg_psf_e2+k3, color='black')\n",
    "ax.set_xlabel(\"EGC g2\")\n",
    "ax.set_ylabel(\"Object g2\")\n",
    "ax.set_ylim((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, correlation functions. We'll use Stile to do this.  Stile's correlation function code wraps TreeCorr, but it has some built-in plotting and data-formatting functions that save us some lines of code here.\n",
    "\n",
    "We need to define the binning for the correlation function.  I've gone for 1/5 of the extent of the catalog in declination for the max distance (possibly large enough to see edge effects anyway), using 20 bins to cover an order of magnitude in angular distance.  We'll also need to rename some columns, since Stile expects specific names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some good bin edges\n",
    "star_catalog = pd.DataFrame(star_catalog).dropna()\n",
    "star_catalog['w'] = np.ones_like(star_catalog['ra'])\n",
    "# Stile requires columns to be named certain things, and also for the data to be in a format where you can\n",
    "# mask entire rows at once.  The Pandas dataframe would probably work, but we'll switch to a numpy recarray,\n",
    "# which is a little more lightweight.\n",
    "star_catalog_stile = star_catalog.rename(columns={'e1': 'g1', 'e2': 'g2', 'psf_e1': 'psf_g1', 'psf_e2': 'psf_g2'}).to_records()\n",
    "# We still need that factor of 2!\n",
    "star_catalog_stile['g1'] /= 2\n",
    "star_catalog_stile['g2'] /= 2\n",
    "min_ra = star_catalog['ra'].min()\n",
    "max_ra = star_catalog['ra'].max()\n",
    "min_dec = star_catalog['dec'].min()\n",
    "max_dec = star_catalog['dec'].max()\n",
    "\n",
    "max_sep = 0.2*(max_dec-min_dec)\n",
    "min_sep = 0.1*max_sep\n",
    "nbins = 20\n",
    "\n",
    "# Make a dict of TreeCorr parameters that we can pass to Stile\n",
    "corrfunc_kwargs = {'ra_units': 'degrees', 'dec_units': 'degrees',\n",
    "                   'min_sep': min_sep, 'max_sep': max_sep, 'sep_units': 'degrees', 'nbins': nbins }\n",
    "\n",
    "rho1 = stile.CorrelationFunctionSysTest(\"Rho1\")\n",
    "rho2 = stile.CorrelationFunctionSysTest(\"Rho2\")\n",
    "rho3 = stile.CorrelationFunctionSysTest(\"Rho3\")\n",
    "rho4 = stile.CorrelationFunctionSysTest(\"Rho4\")\n",
    "rho5 = stile.CorrelationFunctionSysTest(\"Rho5\")\n",
    "\n",
    "rho1_res = rho1(star_catalog_stile, **corrfunc_kwargs)\n",
    "rho2_res = rho2(star_catalog_stile, **corrfunc_kwargs)\n",
    "rho3_res = rho3(star_catalog_stile, **corrfunc_kwargs)\n",
    "rho4_res = rho4(star_catalog_stile, **corrfunc_kwargs)\n",
    "rho5_res = rho5(star_catalog_stile, **corrfunc_kwargs)\n",
    "\n",
    "# When you correct correlation functions for additive and multiplicative bias, the additive is generally stable\n",
    "# and can be done per-object, while the multiplicative is not stable and should be done in ensemble.  Since we measured\n",
    "# m and c from the whole ensemble, though, we can just do the subtraction and division right now for m and c, and \n",
    "# because the alpha term is additive, we can do that per-object right now, too.  And again, we need to flip\n",
    "# the e1 direction to make it comparable to the EGC.\n",
    "\n",
    "merged_table['e1_object_prime'] = -(merged_table['e1_object']/2 - k3 - k2*merged_table['psf_e1'])/(1+k1)\n",
    "merged_table['e2_object_prime'] = (merged_table['e2_object']/2 - k3 - k2*merged_table['psf_e2'])/(1+k1)\n",
    "merged_table['w'] = np.ones_like(merged_table['ra_egc'])\n",
    "\n",
    "merged_table_stile_obj = merged_table.rename(columns={'e1_object_prime': 'g1', 'e2_object_prime': 'g2', 'ra_object': 'ra', 'dec_object': 'dec'}).to_records()\n",
    "object_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "object_corrfunc_res = object_corrfunc('gg', merged_table_stile_obj, **corrfunc_kwargs)\n",
    "\n",
    "merged_table_stile_egc = merged_table.rename(columns={'e1_egc': 'g1', 'e2_egc': 'g2', 'ra_egc': 'ra', 'dec_egc': 'dec'}).to_records()\n",
    "egc_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "egc_corrfunc_res = egc_corrfunc('gg', merged_table_stile_egc, **corrfunc_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory\n",
    "del star_catalog_stile\n",
    "del merged_table_stile_egc\n",
    "del merged_table_stile_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these functions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the interface presented here will change in the near future to rho1_res.plot()\n",
    "fig = rho1.plot(rho1_res)\n",
    "fig.suptitle('Rho 1')\n",
    "plt.clf()\n",
    "\n",
    "fig = rho2.plot(rho2_res)\n",
    "fig.suptitle('Rho 2')\n",
    "plt.clf()\n",
    "\n",
    "fig = rho3.plot(rho3_res)\n",
    "fig.suptitle('Rho 3')\n",
    "plt.clf()\n",
    "\n",
    "fig = rho4.plot(rho4_res)\n",
    "fig.suptitle('Rho 4')\n",
    "plt.clf()\n",
    "\n",
    "fig = rho5.plot(rho5_res)\n",
    "fig.suptitle('Rho 5')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = object_corrfunc.plot(object_corrfunc_res)\n",
    "fig.suptitle('Image-derived correlation function')\n",
    "plt.clf()\n",
    "\n",
    "fig = egc_corrfunc.plot(egc_corrfunc_res)\n",
    "fig.suptitle('EGC-derived correlation function')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following __[Jarvis et al (2015)](https://ui.adsabs.harvard.edu/#abs/arXiv:1507.05603)__, we define the correction to the correlation function as\n",
    "\n",
    "$$ \\delta \\xi_+(\\theta) = 2 \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}} \\frac{\\delta T_{\\rm PSF}}{T_{\\rm PSF}}\\right\\rangle \\xi_+(\\theta)  +  \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}} \\right\\rangle^2 \\rho_1(\\theta) - \\alpha  \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}}\\right\\rangle \\rho_2(\\theta) +  \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}} \\right\\rangle^2 \\rho_3 (\\theta) +  \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}}\\right\\rangle^2 \\rho_4(\\theta) - \\alpha  \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}}\\right\\rangle \\rho_5(\\theta) $$\n",
    "\n",
    "T represents the intensity-weighted second moment of the radius, called $R^2$ in an earlier paper by Paulin-Henriksson et al. (2008).  Handily for us, the ratio of T given above is what's called resolution in the reGaussianization pipeline used for shape measurement by the DM stack.  Following Jarvis et al, we'll approximate that first expectation value as the multiplication of two expectation values:\n",
    "\n",
    "$$\\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}} \\frac{\\delta T_{\\rm PSF}}{T_{\\rm PSF}}\\right\\rangle = \\left\\langle \\frac{T_{\\rm PSF}}{T_{\\rm gal}}\\right\\rangle \\left\\langle \\frac{\\delta T_{\\rm PSF}}{T_{\\rm PSF}}\\right\\rangle $$\n",
    "\n",
    "since we can't measure  the PSF modeling error at the locations of galaxies, and we can't measure galaxy size at the locations of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xip_object = object_corrfunc_res['xip']/(1+2*k1)\n",
    "# We don't have the galaxy shapes directly, but resolution, which we *do* have, is 1-TPSF/Tgal\n",
    "tpsf_tgal = np.mean(1-merged_table['ext_shapeHSM_HsmShapeRegauss_resolution'])\n",
    "tpsf_tgal_deltatpsf = tpsf_tgal*np.mean((star_catalog['psf_sigma']-star_catalog['sigma'])/star_catalog['sigma'])\n",
    "\n",
    "delta_xip = ( 2*tpsf_tgal_deltatpsf * xip_object + tpsf_tgal**2*(rho1_res['xip'] + rho3_res['xip'] + rho4_res['xip'])\n",
    "             - k2*tpsf_tgal*(rho2_res['xip']+rho5_res['xip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the comparison.  For ease of reading the plot, we're only going to plot the errorbars for the corrected object-based shape-shape correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xip_gc = egc_corrfunc_res['xip']\n",
    "xip_object_corrected = (xip_object + delta_xip)\n",
    "\n",
    "xi_err_sq = (object_corrfunc_res['sigma_xi']**2*(1+4*tpsf_tgal_deltatpsf**2) + \n",
    "             tpsf_tgal**4*(rho1_res['sigma_xi']**2 + rho3_res['sigma_xi']**2 + rho4_res['sigma_xi']**2) -\n",
    "             k2**2*tpsf_tgal**2*(rho2_res['sigma_xi']**2+rho5_res['sigma_xi']**2))\n",
    "\n",
    "x = egc_corrfunc_res['meanR [deg]']\n",
    "x_edges = np.concatenate(([x[0]**2/x[1]], np.sqrt(x[:1]*x[1:]), [x[-1]**2/x[-2]]))\n",
    "x_err = [x-x_edges[:-1], x_edges[1:]-x]\n",
    "plt.figure(figsize=(2*figsize_x, 2*figsize_y))\n",
    "plt.plot(x, np.abs(egc_corrfunc_res['xip']), label=\"Truth\", color=\"C0\") \n",
    "plt.errorbar(x, np.abs(xip_object_corrected), yerr=np.sqrt(xi_err_sq),\n",
    "             label=\"object, corrected\", color='C1')\n",
    "plt.plot(x, np.abs(xip_object), \n",
    "             label=\"object\", color='C2')\n",
    "plt.plot(x, np.abs(2*tpsf_tgal_deltatpsf*xip_object), \n",
    "             label=r\"$\\xi_+$ error term\", color='C3')\n",
    "plt.plot(x, np.abs(tpsf_tgal**2*rho1_res['xip']), \n",
    "             label=r\"$\\rho_1$ error term\", color='C4')\n",
    "plt.plot(x, np.abs(k2*tpsf_tgal*rho2_res['xip']), \n",
    "             label=r\"$\\rho_2$ error term\", color='C5')\n",
    "plt.plot(x, np.abs(tpsf_tgal**2*rho3_res['xip']), \n",
    "             label=r\"$\\rho_3$ error term\", color='C6')\n",
    "plt.plot(x, np.abs(tpsf_tgal**2*rho4_res['xip']), \n",
    "             label=r\"$\\rho_4$ error term\", color='C7')\n",
    "plt.plot(x, np.abs(k2*tpsf_tgal*rho5_res['xip']), \n",
    "             label=r\"$\\rho_5$ error term\", color='C8')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"R [deg]\")\n",
    "plt.ylabel(r\"$\\xi_+$\")\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask, what effect does selection have?  These previous plots were measured using only matched objects--but not all objects were matched.  Let's compute some correlation functions using *all* the objects in the given sky area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egc_catalog['w'] = np.ones_like(egc_catalog['ra_true'])\n",
    "for old, new in [('ra_true', 'ra'), ('dec_true', 'dec'), ('ellipticity_1_true', 'g1'), ('ellipticity_2_true', 'g2')]:\n",
    "    egc_catalog[new] = egc_catalog[old]\n",
    "egc_catalog_stile = np.rec.fromarrays(egc_catalog.values(), names=list(egc_catalog.keys()))\n",
    "egc_all_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "egc_all_corrfunc_res = egc_all_corrfunc('gg', egc_catalog_stile, **corrfunc_kwargs)\n",
    "\n",
    "galaxy_catalog['w'] = np.ones_like(galaxy_catalog['ra'])\n",
    "for old, new in [('ext_shapeHSM_HsmShapeRegauss_e1', 'g1'), ('ext_shapeHSM_HsmShapeRegauss_e2', 'g2')]:\n",
    "    galaxy_catalog[new] = galaxy_catalog[old]\n",
    "galaxy_catalog_stile = np.rec.fromarrays(galaxy_catalog.values(), names=list(galaxy_catalog.keys()))\n",
    "galaxy_catalog_stile['g1'] = -(galaxy_catalog_stile['g1']/2 - k3 - k2*galaxy_catalog_stile['psf_e1'])/(1+k1)\n",
    "galaxy_catalog_stile['g2'] = (galaxy_catalog_stile['g2']/2 - k3 - k2*galaxy_catalog_stile['psf_e2'])/(1+k1)\n",
    "object_all_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "object_all_corrfunc_res = object_all_corrfunc('gg', galaxy_catalog_stile, **corrfunc_kwargs)\n",
    "\n",
    "plt.figure(figsize=(2*figsize_x, 2*figsize_y))\n",
    "plt.errorbar(x, object_corrfunc_res['xip'], yerr=object_corrfunc_res['sigma_xi'], label=\"Matched DM objects\")\n",
    "plt.errorbar(x, object_all_corrfunc_res['xip'], yerr=object_all_corrfunc_res['sigma_xi'], label=\"All DM objects\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"R [deg]\")\n",
    "plt.ylabel(r\"$\\xi_+$\")\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.figure(figsize=(2*figsize_x, 2*figsize_y))\n",
    "plt.errorbar(x, egc_corrfunc_res['xip'], yerr=egc_corrfunc_res['sigma_xi'], label=\"Matched EGC objects\")\n",
    "plt.errorbar(x, egc_all_corrfunc_res['xip'], yerr=egc_all_corrfunc_res['sigma_xi'], label=\"All EGC objects\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"R [deg]\")\n",
    "plt.ylabel(r\"$\\xi_+$\")\n",
    "plt.xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at what shape noise does to our measurement, since we have the true shears.  To make this comparison, we have one more coefficient to care about.  We care about the response of the ensemble of shapes to an applied shear, and shears do not add linearly.  This difference is captured by what is usually called the _responsivity_, which (in the simplest case) depends on the per-component RMS distortion:\n",
    "\n",
    "$$ \\mathcal{R} \\approx 1-e_{\\rm rms}^2 $$\n",
    "\n",
    "We would want this to be weighted if we were weighting galaxies, but in this tutorial, we are not.  We will need to apply this correction to our correlation functions for a proper comparison to theory.  To capture only the effects of shape noise (not measurement error or anything else), we'll compare the EGC catalog to the true shears in the EGC directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsivity = 1-np.mean(np.concatenate([egc_catalog_stile['g1']**2, egc_catalog_stile['g2']**2]))\n",
    "print(\"Responsivity = {}\".format(responsivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake the catalog again...\n",
    "egc_catalog_stile['g1'] /= responsivity\n",
    "egc_catalog_stile['g2'] /= responsivity\n",
    "egc_all_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "egc_all_corrfunc_res = egc_all_corrfunc('gg', egc_catalog_stile, **corrfunc_kwargs)\n",
    "\n",
    "egc_catalog_true = egc_catalog_stile.copy()\n",
    "egc_catalog_true['g1'] = egc_catalog_true['shear_1']\n",
    "egc_catalog_true['g2'] = egc_catalog_true['shear_2_treecorr']\n",
    "egc_true_corrfunc = stile.CorrelationFunctionSysTest()\n",
    "egc_true_corrfunc_res = egc_true_corrfunc('gg', egc_catalog_true, **corrfunc_kwargs)\n",
    "\n",
    "plt.figure(figsize=(2*figsize_x, 2*figsize_y))\n",
    "plt.errorbar(x, egc_true_corrfunc_res['xip'], yerr=egc_true_corrfunc_res['sigma_xi'], label=\"True shears\")\n",
    "plt.errorbar(x, egc_all_corrfunc_res['xip'], yerr=egc_all_corrfunc_res['sigma_xi'], label=\"Ellipticities\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"R [deg]\")\n",
    "plt.ylabel(r\"$\\xi_+$\")\n",
    "#plt.yscale('symlog', linthreshy=1.E-4)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-stack-dev",
   "language": "python",
   "name": "desc-stack-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
