{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIA on DC2: Using Run2.1i and matching diaSrc flux averages and the average flux in diaObj\n",
    "\n",
    "Owner: Bruno S.  (Duke U.); RenÃ©e H. (Toronto)\n",
    "Date: 24/01/2020\n",
    "\n",
    "### Brief introduction\n",
    "\n",
    "Difference image analysis is the standard technique to uncover variability and transient events on images. Here we show some work done with the SN sample present in WFD.\n",
    "\n",
    "In this analysis we will find out what does the Run2.1i DIA dataset looks like.  \n",
    "For this we will use a set of templates built for the purpouse as well as difference images already calculated, their association catalogs, and see what we can find.\n",
    "\n",
    "Content here can be summarized as:\n",
    "1. Load DIAObject table and select an object\n",
    "2. Get the diaSrc IDs from the DIAObject table for that object\n",
    "3. Use a magic trick to get the dataIds that allow us to load the diaSrc catalogs for each observation of the source.  \n",
    "    These are the measurements from the detections on each image.  They are _not_ the forced photometry on all available images.\n",
    "4. Load diaSrc values for each detection.\n",
    "5. Calibrate the diaSrc instFlux measurements\n",
    "6. Compare the average flux over all diaSrcs with the mean flux in the diaObj entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import astropy.coordinates as SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "from astropy import time\n",
    "from collections import OrderedDict as Odict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.geom as afwGeom\n",
    "import lsst.geom as geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.afw.geom import makeSkyWcs\n",
    "from lsst.daf.persistence import Butler\n",
    "from lsst.obs.lsst.imsim import ImsimMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_src(src_id):\n",
    "    visit_id = (src_id>>26)//1000\n",
    "    det_id = (src_id>>26)%(1000*visit_id)\n",
    "    return visit_id, det_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibrated_values(aflux, source_row, photcal):\n",
    "    \"\"\"\n",
    "    Obtain the calibration using the DM tools for that particular image\n",
    "    \n",
    "    This way of converting fluxes to magnitudes is aware of the zero points\n",
    "    of the image as well as the correct calculation.\n",
    "    \n",
    "    We can obtain then, magnitudes and fluxes in NJy\n",
    "    \"\"\"\n",
    "    flux, err = source_row[aflux], source_row[aflux+'Err'] \n",
    "    #print(flux, err)\n",
    "    cal_mag = photcal.instFluxToMagnitude(flux, err)\n",
    "    cal_flux = photcal.instFluxToNanojansky(flux, err)\n",
    "    return cal_mag, cal_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes_to_calibrate = ['base_PsfFlux_instFlux',\n",
    "                       'ip_diffim_forced_PsfFlux_instFlux',\n",
    "                       'base_CircularApertureFlux_3_0_instFlux',\n",
    "                       'base_CircularApertureFlux_4_5_instFlux',\n",
    "                       'base_CircularApertureFlux_6_0_instFlux',\n",
    "                       'base_CircularApertureFlux_9_0_instFlux',\n",
    "                       'base_CircularApertureFlux_12_0_instFlux', \n",
    "                       'base_CircularApertureFlux_17_0_instFlux', \n",
    "                       'base_CircularApertureFlux_25_0_instFlux']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a butler for the difference image dataset for Run2.1i\n",
    "\n",
    "This dataset has been built through several steps, and the following graph shows roughly the steps involved.\n",
    "\n",
    "<img src='./plots/processing_schema.png' style=\"height:550px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexprepo = '/global/cscratch1/sd/desc/DC2/data/Run2.1i/rerun/calexp-v1' \n",
    "b = Butler(calexprepo)\n",
    "skymap = b.get('deepCoadd_skyMap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `calexp` repo we can ge the `skyMap` that is useful for analyzing the `tract` and `patch` to use. \n",
    "\n",
    "In this case we are going to work with tract-patch (from now on I will refer to this as `t+p`) `4431`,  `(1, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract = 4432\n",
    "patch = (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_info = skymap[tract]\n",
    "tract_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_patch_box = tract_info.getPatchInfo(patch).getOuterBBox()\n",
    "tract_patch_pos_list = tract_patch_box.getCorners()\n",
    "# Cast to Point2D, because pixelToSky below will refuse to work with a Point2I object.\n",
    "tract_patch_pos_list = [afwGeom.Point2D(tp) for tp in tract_patch_pos_list]\n",
    "\n",
    "wcs = tract_info.getWcs()\n",
    "corners = wcs.pixelToSky(tract_patch_pos_list)\n",
    "corners = np.array([[c.getRa().asDegrees(), c.getDec().asDegrees()] for c in corners])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = corners[:, 0]\n",
    "dec = corners[:, 1]\n",
    "min_ra, max_ra = np.min(ra), np.max(ra)\n",
    "min_dec, max_dec = np.min(dec), np.max(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coordinate borders for patch\\n  {:3.2f} < R.A. < {:3.2f} \\n {:3.2f} < Dec < {:3.2f}'.format(min_ra, max_ra, min_dec, max_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, we make the calculation of the approximated area of this patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ra, delta_dec = max_ra-min_ra, max_dec-min_dec\n",
    "area = np.cos(np.deg2rad(min_dec+delta_dec*0.5))*np.deg2rad(delta_ra)*np.deg2rad(delta_dec)*180.*180./(np.pi**2.)*u.deg*u.deg\n",
    "print('Area = {:4.3f} deg^2, or {:3.2f} arcmin^2'.format(area, area.to(u.arcmin**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the data id for this patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpId = {'tract': 4432, 'patch': '4,3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now acces to the repo of data we have for the difference images at the latest stage, the forced photometry.\n",
    "\n",
    "Using this butler we can access data from every parent repo, ranging from the `calexp` until the forced photometry results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_repo = '/global/cscratch1/sd/bos0109/templates_rect'\n",
    "diarepo = template_repo + '/rerun/diff_rect'\n",
    "assocrepo = diarepo + '/rerun/assoc_sha'\n",
    "forcerepo = assocrepo + '/rerun/forcedPhot' \n",
    "tmprepo = template_repo + '/rerun/multiband'\n",
    "\n",
    "diabutler = Butler(forcerepo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check, which data types can we obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp = diabutler.get('calexp', visit=2340, detector=54)\n",
    "calexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaMapper = diabutler._getDefaultMapper()\n",
    "mapper = diaMapper(root=forcerepo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "#print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to ask for the `diaSrc` detections, these are the individual sources that come from the difference images.\n",
    "Let's ask for similar datatypes stored in our shortlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[datatype for datatype in shortlist if 'dia' in datatype]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask for the `deepDiff_diaSrc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabutler.getKeys('deepDiff_diaSrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for the butler to be able to gives our dataset we need to specify some of the id components above.\n",
    "\n",
    "We have several files in the repo, and we can look for a visit and detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/deepDiff/v00159479-fg/R03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = [diabutler.get('deepDiff_diaSrc', visit=159479, detector=det).asAstropy().to_pandas() \n",
    "        for det in [19, 20, 22, 23, 25, 26]]\n",
    "srcs = pd.concat(srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a better way to do this, involving the Butler and a native functionality called `subset`.  \n",
    "\n",
    "This allow us to use a partial `dataId`, and we can afterwards chose what we want to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'deepDiff_diaSrc'\n",
    "\n",
    "# A visit number we happen to know\n",
    "# We pick raft 3 just to reduce the number of dataIds we're dealing with\n",
    "partial_data_id = {'visit': 159479, 'raftName': 'R03'}  \n",
    "\n",
    "data_refs = diabutler.subset(datasetType=dataset_type, dataId=partial_data_id)\n",
    "\n",
    "data_ids = [dr.dataId for dr in data_refs\n",
    "            if diabutler.datasetExists(datasetType=dataset_type,\n",
    "                                       dataId=dr.dataId)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the resulting `data_ids` list contents, and find that they are consistent with our previous result inspecting the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(did) for did in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = [diabutler.get('deepDiff_diaSrc', dataId=did).asAstropy().to_pandas() for did in data_ids]\n",
    "srcs = pd.concat(srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the table is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(srcs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the positions of the sources, and the patch borders as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.rad2deg(srcs['coord_ra']), np.rad2deg(srcs['coord_dec']), \n",
    "            c=np.log10(srcs['base_PsfFlux_instFlux']))\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.grid()\n",
    "plt.vlines(x=[max_ra, min_ra], ymin=min_dec, ymax=max_dec)\n",
    "plt.hlines(y=[max_dec, min_dec], xmin=min_ra, xmax=max_ra)\n",
    "plt.gca().set_aspect(1./np.cos(np.mean([max_dec, min_dec])))\n",
    "plt.gca().invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple histogram of its fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making lightcurves\n",
    "\n",
    "In order to make lightcurves we will need to use the `diaObject` table. Let's open one of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = diabutler.get('deepDiff_diaObject', dataId=tpId).asAstropy().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are several columns for each one of these `diaObject` and they show information that can have empty information or important data in them.  \n",
    "This depends in the group of `diaSrc` that each `diaObject` is associating. The column which knows about how many observations this is grouping is `nobs`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs.nobs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(objs.nobs, bins=range(50), log=True, histtype='step', lw=2, color='k')\n",
    "plt.xlabel('Number of DIASources in DIAObject')\n",
    "plt.ylabel('# DIAObjects / bin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some of them which actually have only 1 `diaSrc`, i.e. no association of sources at all.\n",
    "\n",
    "If we want to know which sources are associated to each `diaObject` we need the table called `diaObjectId`, which we ask for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objid = diabutler.get('deepDiff_diaObjectId', dataId=tpId).toDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the line above fails, might be due to different versions of the stack. The `diaObjectId` is a relatively new feature, and its creation is possible using the latest version of `dia_pipe`.\n",
    "\n",
    "An alternative way of accessing the table is through the parquet tables, where this information comes from. This files are located in the associated piece of the repo directory tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/rerun/assoc_sha/deepDiff/diaObject/4432/4,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/deepDiff/v00002340-fu/R13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objId = pd.read_parquet('/global/cscratch1/sd/bos0109/templates_rect/rerun/diff_rect/rerun/assoc_sha/deepDiff/diaObject/4432/4,3/diaObjectId-4432-4,3.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objId['nobs'] = objId.diaSrcIds.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objId.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell from the first rows, the table only hosts two columns and the `diaSrcIds` contains a list of the `diaSrcs` that are linked to a particular `diaObject`.\n",
    "We also added the `nobs` column which is important to have it here too.\n",
    "Let's work with the subset of `diaObject` with more sources associated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_objs = objId[objId.nobs>=30]\n",
    "#subset_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we are pulling out one object from the subset of onjects with lots of sources\n",
    "i_row=0\n",
    "print('the ID is : ', int(subset_objs.iloc[i_row]['diaObjectId']))\n",
    "\n",
    "# now we take the object info from the object id list corresponding to that object ID\n",
    "selected_obj = objs[objs['id'] == int(subset_objs.iloc[i_row]['diaObjectId'])]\n",
    "\n",
    "#now I want to take the sources that have the same linked object id - so I can pull all the srcs with the same object ID\n",
    "objdrow = objId[objId.diaObjectId==int(selected_obj['id'])]\n",
    "\n",
    "# Now I have a list of all sources IDs from this object row\n",
    "srcids = objdrow['diaSrcIds'].tolist()[0]\n",
    "\n",
    "# Now I'm going to go to the visits and pull out the info e.g. flux from the sources in that image. \n",
    "#The default visit and detecor visit queries will give me all src again though, \n",
    "# so need to pull out the right one to compute the mean flux *across* visits\n",
    "\n",
    "visits=[]\n",
    "detectors=[]\n",
    "for asrcid in srcids:\n",
    "    visit_id, det_id = get_id_from_src(asrcid)\n",
    "    \n",
    "    visits.append(visit_id)\n",
    "    detectors.append(det_id)\n",
    "\n",
    "srcidstab = pd.DataFrame(np.array([srcids, visits, detectors]).T, columns=['srcid', 'visit', 'det'])    \n",
    "    \n",
    "for idx, anid, avisit, adet in srcidstab.itertuples():\n",
    "    # Here we just ask for the sources tab and the \n",
    "    # calibration for the image which they come from\n",
    "    sources_tab = diabutler.get('deepDiff_diaSrc', \n",
    "                                visit=avisit, detector=adet).asAstropy().to_pandas()\n",
    "    # get the calibration tool\n",
    "    photcal = diabutler.get('deepDiff_differenceExp_photoCalib', \n",
    "                           visit=avisit, detector=adet)\n",
    "    \n",
    "    # get the calexp to obtain MJD and filter\n",
    "    calexp = diabutler.get('calexp', visit=int(avisit), detector=int(adet))\n",
    "    date_mjd = calexp.getInfo().getVisitInfo().getDate().get()\n",
    "    fltr = calexp.getInfo().getFilter().getCanonicalName()\n",
    "        # include this in the source table \n",
    "    sources_tab['filter'] = fltr\n",
    "    sources_tab['MJD'] = date_mjd\n",
    "    \n",
    "    \n",
    "    source_row = sources_tab[sources_tab.id == anid].copy()\n",
    "    \n",
    "    #print(fluxes_to_calibrate)\n",
    "    for aflux in fluxes_to_calibrate:\n",
    "        #print(aflux, source_row)\n",
    "        cal_mag, cal_flux = get_calibrated_values(aflux, source_row, photcal)\n",
    "        source_row[aflux+'_calMag'] = cal_mag.value\n",
    "        source_row[aflux+'_calMagErr'] = cal_mag.error\n",
    "        source_row[aflux+'_nJy'] = cal_flux.value\n",
    "        source_row[aflux+'_nJyErr'] = cal_flux.error\n",
    "\n",
    "\n",
    "for aflux in fluxes_to_calibrate:\n",
    "    \n",
    "    objdrow[aflux+'_meanMag'] = np.mean(source_row[aflux+'_calMag'])\n",
    "\n",
    "    #srcidstab = pd.DataFrame(np.array([srcids, visits, detectors]).T, columns=['srcid', 'visit', 'det'])\n",
    "#print(srcidstab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_row\n",
    "#objdrow\n",
    "#keepObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = diabutler.get('deepDiff_diaObject', dataId=tpId).asAstropy().to_pandas()\n",
    "keepObjs = objs[objs.nobs>=30]\n",
    "#print(keepObjs.iloc[i_row]['base_PsfFlux_instFlux_Mean_u'], objdrow.iloc[i_row]['base_CircularApertureFlux_3_0_instFlux_meanMag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-dia",
   "language": "python",
   "name": "desc-dia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
